{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uqt9xdfOyWso"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "\n",
        "input_folder = '/content/drive/My Drive/ocr_output_read'\n",
        "text_files = os.listdir(input_folder)\n",
        "documents = []\n",
        "\n",
        "\n",
        "recording_details_df = pd.DataFrame(columns=[\"filename\", \"header\", \"book_num\", \"page_num\", \"recording_date\", \"doc_date\", \"signature_date\",\"effective_date\", \"document_number\", \"consideration\", \"taxes\"])\n",
        "\n",
        "num_matches = 0\n",
        "count = 0\n",
        "for i, text_file in enumerate(text_files):\n",
        "  with open(os.path.join(input_folder, text_file), \"r\", encoding=\"utf-8\") as file:\n",
        "\n",
        "    print(text_file)\n",
        "    content = file.read()\n",
        "    valid_matches = extract_document_info(text_file, content)\n",
        "    header, book_num, page_num, recording_date, all_doc_num_matches, doc_date, signature_date, effective_date = valid_matches\n",
        "\n",
        "\n",
        "    if doc_date:\n",
        "      doc_date = doc_date.strftime(\"%m/%d/%Y\")\n",
        "    if signature_date:\n",
        "      signature_date = signature_date.strftime(\"%m/%d/%Y\")\n",
        "    if effective_date:\n",
        "      effective_date = effective_date.strftime(\"%m/%d/%Y\")\n",
        "\n",
        "\n",
        "    if all_doc_num_matches is None:\n",
        "      valid_doc_num_match = []\n",
        "\n",
        "    else:\n",
        "      valid_doc_num_match = [vdnm for vdnm in all_doc_num_matches]\n",
        "\n",
        "    if recording_date is None:\n",
        "\n",
        "      print(\"date not found\")\n",
        "      num_matches +=1\n",
        "\n",
        "    doc_num_matches = list(map(lambda s: re.sub(r\"\\s+\", \"\", s), valid_doc_num_match))\n",
        "    counter = Counter(doc_num_matches)\n",
        "    doc_num_matches = [item for item, count in counter.most_common()]\n",
        "\n",
        "    matches = [x for x in doc_num_matches if str(x).startswith(\"202\")]\n",
        "    result = [matches[0]] if len(matches) == 1 else doc_num_matches\n",
        "\n",
        "\n",
        "    # print(\"document number:\")\n",
        "    # print(doc_num_matches)\n",
        "\n",
        "\n",
        "\n",
        "    consideration = find_sales_amount(content)\n",
        "    # print(consideration)\n",
        "    taxes = find_tax_fee(content)\n",
        "    print(taxes)\n",
        "\n",
        "    # print(book_num, \", \", page_num)\n",
        "    # print(consideration)\n",
        "    # print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "    if valid_matches is not None:\n",
        "      recording_details_df.loc[i] = [text_file, header, book_num, page_num, recording_date, doc_date, signature_date, effective_date, result, consideration, taxes]\n",
        "\n",
        "    content = clean_text(content, patterns)\n",
        "    documents.append([text_file, content])\n",
        "\n",
        "print(\"total matches: \", num_matches)\n",
        "\n",
        "\n",
        "def calculate_attention_mask(seq_length, input_len):\n",
        "  return [1]*seq_length + [0]*(input_len - seq_length)\n",
        "\n",
        "\n",
        "def prepare_text_data_for_inference(documents):\n",
        "\n",
        "  inference_text = []\n",
        "  input_len = 512\n",
        "  start_token_id = [101]\n",
        "  end_token_id = [102]\n",
        "  overlap_window = 50\n",
        "\n",
        "  for fname, document in documents:\n",
        "    tokenized_text = tokenizer(document, truncation=False, return_offsets_mapping=False)\n",
        "    input_id = tokenized_text[\"input_ids\"]\n",
        "\n",
        "    if len(input_id) > input_len - 2:\n",
        "\n",
        "      inference_text.append([input_id[0:input_len - 1] + end_token_id,\n",
        "                  calculate_attention_mask(input_len, input_len),\n",
        "                  0,\n",
        "                  fname\n",
        "                  ])\n",
        "\n",
        "\n",
        "      start = input_len - overlap_window - 1\n",
        "      while start < len(input_id) - input_len + 1 :\n",
        "\n",
        "          inference_text.append([start_token_id + input_id[start:start + input_len - 2] + end_token_id,\n",
        "                      calculate_attention_mask(input_len, input_len),\n",
        "                      0,\n",
        "                      fname\n",
        "                      ])\n",
        "          start = start + input_len - overlap_window - 2\n",
        "\n",
        "      inference_text.append([start_token_id + input_id[start:len(input_id)],\n",
        "            calculate_attention_mask(len(input_id) + 1 - start, input_len),\n",
        "            1,\n",
        "            fname\n",
        "            ])\n",
        "\n",
        "    else:\n",
        "\n",
        "      inference_text.append([input_id,\n",
        "                  calculate_attention_mask(len(input_id) + 2, input_len),\n",
        "                  1,\n",
        "                  fname\n",
        "                    ])\n",
        "  return inference_text\n",
        "\n",
        "\n",
        "def prepare_df_for_inference(data):\n",
        "\n",
        "  input_ids = [entry[0] for entry in data]\n",
        "  attention_mask = [entry[1] for entry in data]\n",
        "  is_last_chunk = [entry[2] for entry in data]\n",
        "  file_name = [entry[3] for entry in data]\n",
        "\n",
        "  df = pd.DataFrame({\n",
        "    \"input_ids\": input_ids,\n",
        "    \"attention_mask\" : attention_mask,\n",
        "    \"is_last_chunk\": is_last_chunk,\n",
        "    \"file_name\": file_name\n",
        "  })\n",
        "\n",
        "  df[\"input_ids\"] = df[\"input_ids\"].apply(lambda x: x + ([0]*(512-len(x))))\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "data = prepare_text_data_for_inference(documents)\n",
        "df = prepare_df_for_inference(data)\n",
        "df.columns\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def perform_inference_on_processed_text(df):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  input_ids = torch.tensor(df[\"input_ids\"].tolist())\n",
        "  attention_mask = torch.tensor(df[\"attention_mask\"].tolist())\n",
        "\n",
        "  batch_size = 8 # Adjust based on memory availability\n",
        "\n",
        "  predictions = []\n",
        "\n",
        "  # Process in batches\n",
        "  for i in range(0, len(input_ids), batch_size):\n",
        "      batch_input_ids = input_ids[i:i+batch_size].to(device)\n",
        "      batch_attention_mask = attention_mask[i:i+batch_size].to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
        "\n",
        "      # Get predictions for the current batch\n",
        "      batch_predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "      predictions.extend(batch_predictions.cpu().tolist())\n",
        "\n",
        "  # Add predictions to DataFrame\n",
        "  df[\"predictions\"] = predictions\n",
        "  return df\n",
        "\n",
        "print(np.sum(df[\"is_last_chunk\"]))\n",
        "df = perform_inference_on_processed_text(df)\n",
        "\n"
      ],
      "metadata": {
        "id": "A42SLmKSykbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def aggregate_predictions(predictions):\n",
        "\n",
        "  all_entities = ['BUYER ORG', 'SELLER ORG', 'APN', 'BUYER NAME', 'SELLER NAME', 'BUYER ADDRESS', 'SELLER ADDRESS']\n",
        "  final_prediction = []\n",
        "\n",
        "  for entity in all_entities:\n",
        "\n",
        "    entity_level_predictions = [p[1] for p in predictions if p[0] == entity]\n",
        "    final_prediction.append([entity, entity_level_predictions])\n",
        "\n",
        "  return final_prediction\n",
        "\n",
        "\n",
        "def view_predictions(df):\n",
        "\n",
        "  predictions_df = pd.DataFrame()\n",
        "  document_level_predictions = []\n",
        "\n",
        "  for i in range(len(df)):\n",
        "      predictions = df.iloc[i][\"predictions\"]\n",
        "\n",
        "      filtered_predictions = np.array(predictions)\n",
        "      predictions_mapped = list(map(lambda x: reverse_entity_mapping[x], filtered_predictions))\n",
        "      predicted_masks, predicted_entity_names = find_entities_in_text(predictions_mapped)\n",
        "      input_ids = np.array(df.iloc[i][\"input_ids\"])\n",
        "\n",
        "      predicted_entities, predicted_text = show_sample(\n",
        "          input_ids, predicted_masks, predicted_entity_names\n",
        "      )\n",
        "\n",
        "      for e, p in zip(predicted_entities, predicted_text):\n",
        "        document_level_predictions.append((e[0],p))\n",
        "\n",
        "      tokens = tokenizer.convert_ids_to_tokens(np.array(input_ids))\n",
        "      original_text = \" \".join(tokens).replace(\" ##\", \"\")\n",
        "\n",
        "      predictions_df_entry = {}\n",
        "      predictions_df_entry[\"IMAGENAME\"] = df.iloc[i][\"file_name\"]\n",
        "\n",
        "      if(df.iloc[i][\"is_last_chunk\"] == 1):\n",
        "\n",
        "        final_prediction = aggregate_predictions(document_level_predictions)\n",
        "        for e, p in final_prediction:\n",
        "\n",
        "          if len(p) == 0:\n",
        "            continue\n",
        "\n",
        "          predictions_df_entry[e] = \" ^ \".join(map(str, p))\n",
        "\n",
        "        predictions_df = pd.concat([predictions_df, pd.DataFrame([predictions_df_entry])], ignore_index=True)\n",
        "        document_level_predictions = []\n",
        "\n",
        "\n",
        "  return predictions_df\n",
        "\n",
        "predictions_df = view_predictions(df)\n",
        "predictions_df = predictions_df.merge(recording_details_df, left_on=\"IMAGENAME\", right_on=\"filename\", how=\"inner\").drop(columns=[\"filename\"])\n",
        "\n",
        "all_files = []\n",
        "for index, row in predictions_df.iterrows():\n",
        "    row_dict = row.dropna().to_dict()  # Convert row to dict after dropping NaNs\n",
        "    imagename = row_dict.pop(\"IMAGENAME\", \"Unknown\")  # Extract IMAGENAME\n",
        "\n",
        "    all_files.append(imagename)\n",
        "    print(f\"{imagename}\")\n",
        "    print(f\"{row_dict}\")  # 'filename' field is no longer here\n",
        "    print(\"\\n\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "ISIuyUIJyxXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_df[\"SELLER ADDRESS\"] = predictions_df[\"SELLER ADDRESS\"].dropna().apply(\n",
        "    lambda x: max(x.split(\"^\"), key=len).strip()\n",
        ")\n",
        "\n",
        "predictions_df[\"BUYER ADDRESS\"] = predictions_df[\"BUYER ADDRESS\"].dropna().apply(\n",
        "    lambda x: max(x.split(\"^\"), key=len).strip()\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zIBJKm6jy2zK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}