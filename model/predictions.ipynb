{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUx3OitA6p3X"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForTokenClassification, BertTokenizer\n",
        "\n",
        "def load_model_and_tokenizer_for_inference():\n",
        "\n",
        "  model = BertForTokenClassification.from_pretrained(\"/content/drive/My Drive/SDE_bert_model\")\n",
        "  tokenizer = BertTokenizer.from_pretrained(\"/content/drive/My Drive/SDE_bert_tokenizer\")\n",
        "\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "\n",
        "  print(\"Model and tokenizer loaded successfully for inference!\")\n",
        "  return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mappings(corrected_mapping):\n",
        "\n",
        "  reverse_entity_mapping = {}\n",
        "  for key, val in corrected_mapping.items():\n",
        "    reverse_entity_mapping[val] = key\n",
        "\n",
        "  base_entity_to_id = {\"BUYER NAME\":0, \"SELLER NAME\":1,\"BUYER ADDRESS\":2, \"BUYER ORG\":3, \"SELLER ORG\":4, \"SELLER ADDRESS\":5, \"APN\":6}\n",
        "  n = len(base_entity_to_id)\n",
        "  base_entity_misclassifications = np.zeros((n,n))\n",
        "  correct_classification = np.zeros(n)\n",
        "\n",
        "\n",
        "  reverse_base_entity_to_id = {value:key for key, value in base_entity_to_id.items()}\n",
        "\n",
        "  return reverse_entity_mapping, base_entity_to_id, reverse_base_entity_to_id, base_entity_misclassifications, correct_classification\n"
      ],
      "metadata": {
        "id": "b7fqFAAoxNNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def find_entities_in_text(labels):\n",
        "    masks = []\n",
        "    entity_names = []\n",
        "\n",
        "    for i, label in enumerate(labels):\n",
        "        if i > 0 and labels[i] != 'O' and labels[i][2:] != labels[i-1][2:]:\n",
        "            entity_end = i\n",
        "            entity_names.append([labels[i][2:]])\n",
        "\n",
        "            while entity_end < len(labels) and labels[entity_end] != 'O':\n",
        "                entity_end += 1\n",
        "\n",
        "            mask = ([1] * (entity_end - i)) + ([0] * (len(labels) - entity_end + i))\n",
        "            mask = np.array(np.roll(mask, shift=i))\n",
        "            masks.append(mask)\n",
        "\n",
        "    return np.array(masks), entity_names\n",
        "\n",
        "\n",
        "\n",
        "def show_sample(input_ids, masks, entity_names):\n",
        "\n",
        "    all_entities = []\n",
        "    reconstructed_text = []\n",
        "    for i in range(len(masks)):\n",
        "\n",
        "        all_entities.append(entity_names[i])\n",
        "        boolean_mask = np.array(masks[i,:], dtype=bool)\n",
        "        tokens = tokenizer.convert_ids_to_tokens(np.array(input_ids)[boolean_mask])\n",
        "        reconstructed_text.append(\" \".join(tokens).replace(\" ##\", \"\"))\n",
        "\n",
        "    return all_entities, reconstructed_text\n",
        "\n"
      ],
      "metadata": {
        "id": "AuRH7DC1xHbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "\n",
        "def add_to_list(data, entity_map, entity_name, value):\n",
        "\n",
        "  if entity_name not in entity_map:\n",
        "\n",
        "    data.append((entity_name, value))\n",
        "    entity_map[entity_name] = len(data) - 1\n",
        "\n",
        "  else:\n",
        "\n",
        "    entity_index = entity_map[entity_name]\n",
        "    original_entry =  data[entity_index]\n",
        "    data[entity_index] = (entity_name, original_entry[1]+value)\n",
        "\n",
        "  return data, entity_map\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def perform_inference(dataset, corrected_mapping):\n",
        "    reverse_entity_mapping, base_entity_to_id, reverse_base_entity_to_id, base_entity_misclassifications, correct_classification = create_mappings(corrected_mapping)\n",
        "\n",
        "    csv_data = []\n",
        "    all_ground_truths = []\n",
        "    all_predictions = []\n",
        "    model.eval()\n",
        "\n",
        "    result = []\n",
        "    actual_document_level_entities = []\n",
        "    predicted_document_level_entities = []\n",
        "    actual_entity_map = {}\n",
        "    predicted_entity_map = {}\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataset:\n",
        "            for index, (sequence_ids, attention_mask, labels) in enumerate(zip(\n",
        "                batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"]\n",
        "            )):\n",
        "                sequence_ids = sequence_ids.to(device).unsqueeze(0)\n",
        "                attention_mask = attention_mask.to(device).unsqueeze(0)\n",
        "                labels = labels.to(device).unsqueeze(0)\n",
        "                is_last_chunk = batch[\"is_last_chunk\"][index].numpy()\n",
        "\n",
        "                outputs = model(input_ids=sequence_ids, attention_mask=attention_mask)\n",
        "                logits = outputs.logits\n",
        "\n",
        "                predictions = torch.argmax(logits, dim=-1).squeeze(0)\n",
        "\n",
        "                valid_indices = labels.squeeze(0) != -100\n",
        "                filtered_predictions = predictions[valid_indices].cpu().numpy()\n",
        "                filtered_labels = labels.squeeze(0)[valid_indices].cpu().numpy()\n",
        "\n",
        "                all_predictions.extend(filtered_predictions)\n",
        "                all_ground_truths.extend(filtered_labels)\n",
        "\n",
        "                predictions_mapped = list(map(lambda x: reverse_entity_mapping[x], filtered_predictions))\n",
        "                labels = labels.tolist()[0]\n",
        "                ground_truth_mapped = list(map(lambda x: reverse_entity_mapping[x], labels))\n",
        "\n",
        "\n",
        "\n",
        "                predicted_masks, predicted_entity_names = find_entities_in_text(predictions_mapped)\n",
        "                predicted_entities, predictions = show_sample(sequence_ids.squeeze(0).cpu().numpy(), predicted_masks, predicted_entity_names)\n",
        "\n",
        "                actual_masks, actual_entity_names = find_entities_in_text(ground_truth_mapped)\n",
        "                actual_entities, ground_truth = show_sample(sequence_ids.squeeze(0).cpu().numpy(), actual_masks, actual_entity_names)\n",
        "\n",
        "\n",
        "                for actual_entity, gt in zip(actual_entities, ground_truth):\n",
        "                  actual_document_level_entities, actual_entity_map = add_to_list(actual_document_level_entities, actual_entity_map, actual_entity[0], gt )\n",
        "\n",
        "                for predicted_entity, prediction in zip(predicted_entities, predictions):\n",
        "                  predicted_document_level_entities, predicted_entity_map = add_to_list(predicted_document_level_entities, predicted_entity_map, predicted_entity[0], prediction)\n",
        "\n",
        "                if is_last_chunk == 1:\n",
        "\n",
        "                  result.append([actual_document_level_entities, predicted_document_level_entities])\n",
        "                  actual_document_level_entities = []\n",
        "                  predicted_document_level_entities = []\n",
        "                  actual_entity_map = {}\n",
        "                  predicted_entity_map = {}\n",
        "\n",
        "\n",
        "                if len(predicted_masks) > 0:\n",
        "                    for predicted_mask, predicted_entity_name, actual_mask, actual_entity_name in zip(predicted_masks, predicted_entity_names, actual_masks, actual_entity_names):\n",
        "                        predicted_entity_id = base_entity_to_id[predicted_entity_name[0]]\n",
        "                        actual_entity_id = base_entity_to_id[actual_entity_name[0]]\n",
        "\n",
        "                        overlapping_indices = [predicted_mask[i] & actual_mask[i] for i, _ in enumerate(predicted_mask)]\n",
        "\n",
        "                        if len(overlapping_indices) >= 0.66 * np.sum(predicted_mask) :\n",
        "\n",
        "                            if predicted_entity_name != actual_entity_name:\n",
        "                                base_entity_misclassifications[predicted_entity_id][actual_entity_id] += 1\n",
        "                            else:\n",
        "                              correct_classification[predicted_entity_id] += 1\n",
        "\n",
        "                        else:\n",
        "                            base_entity_misclassifications[predicted_entity_id][actual_entity_id] += 1\n",
        "\n",
        "\n",
        "    target_names = [reverse_entity_mapping[i] for i in range(len(reverse_entity_mapping))]\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_ground_truths, all_predictions, target_names=target_names))\n",
        "\n",
        "    return result, base_entity_misclassifications, correct_classification, base_entity_to_id, reverse_base_entity_to_id\n"
      ],
      "metadata": {
        "id": "SsWiiP-8xRex"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}