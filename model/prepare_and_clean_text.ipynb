{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRZIEVWvTKcl",
        "outputId": "59bcc8df-473c-4b73-f5bf-2310350ccd30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def read_data_from_json(file_path):\n",
        "\n",
        "  with open(file_path, 'r') as file:\n",
        "      deeds_data = json.load(file)\n",
        "\n",
        "  classes = deeds_data['classes']\n",
        "  annotations = deeds_data['annotations']\n",
        "\n",
        "  return classes, annotations\n"
      ],
      "metadata": {
        "id": "0xJV2If5TZq1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def read_data_from_second_json_file(file_path):\n",
        "\n",
        "  subfolders = os.listdir(file_path)\n",
        "\n",
        "  valid_entities = {\"BUYER NAME\", \"SELLER NAME\", \"BUYER ADDRESS\", \"BUYER NONINDIVIDUAL NAME\", \"SELLER NONINDIVIDUAL NAME\", \"PROPERTY ADDRESS\", \"ASSESSOR PARCEL NUMBER\"}\n",
        "\n",
        "  labelled_text = []\n",
        "  for subfolder in subfolders:\n",
        "    subfolder_path = os.path.join(file_path, subfolder)\n",
        "    json_files = os.listdir(subfolder_path)\n",
        "    for json_file in json_files:\n",
        "\n",
        "      full_file_path = os.path.join(subfolder_path, json_file)\n",
        "      with open(full_file_path, 'r') as file:\n",
        "        deeds_data = json.load(file)\n",
        "\n",
        "      classes = deeds_data['classes']\n",
        "      annotations = deeds_data['annotations']\n",
        "      for text, entities in annotations:\n",
        "\n",
        "        entities_present_in_text = entities[\"entities\"]\n",
        "        updated_entities = entities_present_in_text[:]\n",
        "\n",
        "        for i, entity_present_in_text in enumerate(entities_present_in_text):\n",
        "\n",
        "          if len(entities_present_in_text) > 0:\n",
        "            if entity_present_in_text[2] == 'SELLER NON INDIVIDUAL NAMES':\n",
        "              updated_entities[i][2] = 'SELLER NONINDIVIDUAL NAME'\n",
        "\n",
        "            if entity_present_in_text[2] == 'BUYER ADDDRESS':\n",
        "              updated_entities[i][2] = 'BUYER ADDRESS'\n",
        "\n",
        "            filtered_entities = list(filter(lambda x: x[2] in valid_entities, updated_entities))\n",
        "\n",
        "        labelled_text.append([text, {\"entities\":filtered_entities}])\n",
        "\n",
        "  return labelled_text\n",
        "\n",
        "\n",
        "\n",
        "file_path = '/content/drive/My Drive/WD-(4129)/'\n",
        "\n",
        "def load_test_data_from_json(file_path , second_file_path):\n",
        "\n",
        "  json_files = os.listdir(file_path)\n",
        "  classes = ['ASSESSOR PARCEL NUMBER', 'PROPERTY ADDRESS', 'BUYER ADDRESS', 'BUYER NAME', 'BUYER NONINDIVIDUAL NAME', 'SELLER NAME', 'SELLER NONINDIVIDUAL NAME']\n",
        "  cleaned_annotations = []\n",
        "  for json_file in json_files:\n",
        "\n",
        "    full_file_path = os.path.join(file_path, json_file)\n",
        "\n",
        "    with open(full_file_path, 'r') as file:\n",
        "      deeds_data = json.load(file)\n",
        "      for text, entity in deeds_data[\"annotations\"]:\n",
        "        cleaned_annotations.append([text, entity])\n",
        "\n",
        "  if second_file_path != None:\n",
        "\n",
        "    second_batch = read_data_from_second_json_file(second_file_path)\n",
        "    cleaned_annotations.extend(second_batch)\n",
        "\n",
        "  print(len(cleaned_annotations))\n",
        "  return classes, cleaned_annotations\n"
      ],
      "metadata": {
        "id": "iqaD0IjITb6w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy # 3.3.0\n",
        "print(spacy.__version__)\n",
        "from sklearn.utils import shuffle\n",
        "from tqdm import tqdm\n",
        "\n",
        "nlp = spacy.blank(\"en\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ54OWziTjcU",
        "outputId": "037547a7-34b3-44a7-f74f-6f42a501117e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.8.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def locate_entity_in_text(annotations, idx):\n",
        "\n",
        "  record = annotations[idx]\n",
        "  text = record[0]\n",
        "  entities = record[1][\"entities\"]\n",
        "\n",
        "  for start_idx, end_idx, ent, in entities:\n",
        "\n",
        "    print(\"entity: \", ent)\n",
        "    print(text[start_idx:end_idx])\n",
        "    print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Bq6a9QEpTlDJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import itertools\n",
        "\n",
        "\n",
        "def get_overlapping_tokens(entity, token_matches):\n",
        "    start_idx, end_idx, name = entity\n",
        "    token_matches = list(token_matches)\n",
        "\n",
        "    overlapping_tokens = [\n",
        "        x for x in token_matches\n",
        "        if (start_idx <= x.start() <= end_idx) ^ (start_idx <= x.end() <= end_idx)\n",
        "    ]\n",
        "\n",
        "    return overlapping_tokens\n",
        "\n",
        "def adjust_entity_boundaries(patterns, annotations):\n",
        "    adjusted_data = []\n",
        "\n",
        "    for text, annotation in annotations:\n",
        "        token_matches = list(itertools.chain.from_iterable(\n",
        "            re.finditer(pattern, text) for pattern in patterns\n",
        "        ))\n",
        "\n",
        "        if token_matches:\n",
        "            updated_entities = {\"entities\": []}\n",
        "            previous_entities = annotation[\"entities\"]\n",
        "\n",
        "            for entity in previous_entities:\n",
        "                start_idx, end_idx, name = entity\n",
        "                overlapping_tokens = get_overlapping_tokens(entity, token_matches)\n",
        "\n",
        "                if overlapping_tokens:\n",
        "                    for token in overlapping_tokens:\n",
        "                        if end_idx - token.start() > token.end() - start_idx:\n",
        "                            start_idx = token.end()\n",
        "                        else:\n",
        "                            end_idx = token.start()\n",
        "\n",
        "                    if start_idx < end_idx:\n",
        "                        updated_entities[\"entities\"].append([start_idx, end_idx, name])\n",
        "                else:\n",
        "\n",
        "                    updated_entities[\"entities\"].append([start_idx, end_idx, name])\n",
        "\n",
        "            adjusted_data.append([text, updated_entities])\n",
        "        else:\n",
        "            adjusted_data.append([text, annotation])\n",
        "\n",
        "    return adjusted_data\n",
        "\n",
        "\n",
        "pattern1 = re.escape(\"<laysep@@##$$>\")\n",
        "pattern2 = re.escape(\"<pagesep@@##$$>\")\n",
        "pattern3 = r\"(?:\\\\u[0-9]{2}[a-z]{2}){3}\"\n",
        "patterns = [pattern1, pattern2, pattern3]\n"
      ],
      "metadata": {
        "id": "Mi9raqrrTrIG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def clean_text(raw_text, patterns):\n",
        "\n",
        "  ct = re.sub(pattern1, \"\", raw_text)\n",
        "  ct = re.sub(pattern2, \"\", ct)\n",
        "  ct = re.sub(pattern3, \"\", ct)\n",
        "\n",
        "  return ct\n",
        "\n",
        "\n",
        "def clean_data(correct_annotations):\n",
        "\n",
        "  cleaned_data = []\n",
        "  for t, e in correct_annotations:\n",
        "\n",
        "    ct = clean_text(t, patterns)\n",
        "    entities = e[\"entities\"]\n",
        "    updated_dict = {}\n",
        "    values = []\n",
        "    for start_idx, end_idx, ent in entities:\n",
        "\n",
        "      raw_text_copy = t[0:start_idx]\n",
        "      cleaned_text = clean_text(raw_text_copy, patterns)\n",
        "      offset = len(raw_text_copy) - len(cleaned_text)\n",
        "      values.append([start_idx-offset, end_idx-offset, ent])\n",
        "\n",
        "    updated_dict[\"entities\"] = values\n",
        "    cleaned_data.append([ct, updated_dict])\n",
        "\n",
        "  return cleaned_data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XKC0d9k7TsZy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.util import filter_spans\n",
        "from tqdm import tqdm\n",
        "from spacy.training import Example\n",
        "from spacy.tokens import DocBin\n",
        "\n",
        "\n",
        "def align_data(cleaned_data):\n",
        "\n",
        "  aligned_data = []\n",
        "  entity_occurence = {}\n",
        "  for text, entities in cleaned_data:\n",
        "\n",
        "    labels = entities[\"entities\"]\n",
        "    doc = nlp.make_doc(text)\n",
        "    values = []\n",
        "    for start, end, name in labels:\n",
        "\n",
        "      if name in entity_occurence:\n",
        "        entity_occurence[name] = entity_occurence[name] + 1\n",
        "      else:\n",
        "        entity_occurence[name] = 1\n",
        "\n",
        "      span = doc.char_span(start, end, label=name, alignment_mode=\"expand\")\n",
        "      if span is not None:\n",
        "        values.append([span.start_char, span.end_char, span.label_])\n",
        "\n",
        "    updated_dict = {\"entities\":values}\n",
        "    aligned_data.append([text, updated_dict])\n",
        "\n",
        "  return aligned_data\n",
        "\n"
      ],
      "metadata": {
        "id": "3tWQqadsTzGV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def label_data(aligned_data):\n",
        "\n",
        "  annotated_bert = []\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "  for text, entities in aligned_data:\n",
        "\n",
        "    encoding = tokenizer(text, truncation=False, return_offsets_mapping=True)\n",
        "\n",
        "    mappings = encoding[\"offset_mapping\"]\n",
        "    input_ids = encoding[\"input_ids\"]\n",
        "    labels = ['O']*len(input_ids)\n",
        "\n",
        "    ents = entities[\"entities\"]\n",
        "    for i, mapping in enumerate(mappings):\n",
        "      for start_idx, end_idx, name in ents:\n",
        "\n",
        "        if mapping == (0,0):\n",
        "          continue\n",
        "\n",
        "        if mapping[0] >= start_idx and mapping[1] <= end_idx:\n",
        "\n",
        "          if mapping[0] == start_idx:\n",
        "            labels[i] = f\"B-{name}\"\n",
        "\n",
        "          else:\n",
        "            labels[i] = f\"I-{name}\"\n",
        "    annotated_bert.append([input_ids, labels])\n",
        "\n",
        "  return annotated_bert, tokenizer\n"
      ],
      "metadata": {
        "id": "vmpdiq4cUAVq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}