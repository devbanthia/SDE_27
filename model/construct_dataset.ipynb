{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WLip1gH1tej"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def calculate_attention_mask(seq_length, input_len):\n",
        "  return [1]*seq_length + [0]*(input_len - seq_length)\n",
        "\n",
        "\n",
        "\n",
        "def prepare_dataset(annotated_bert):\n",
        "  data = []\n",
        "\n",
        "  input_len = 512\n",
        "  overlap_window = 50\n",
        "\n",
        "  start_token_id = [101]\n",
        "  end_token_id = [102]\n",
        "  outside_label = ['O']\n",
        "\n",
        "  for input_id, label in annotated_bert:\n",
        "\n",
        "      if len(label) > input_len - 2:\n",
        "\n",
        "          data.append([input_id[0:input_len - 1] + end_token_id,\n",
        "                      label[0:input_len - 1] + outside_label,\n",
        "                      calculate_attention_mask(input_len, input_len),\n",
        "                      0\n",
        "                      ])\n",
        "\n",
        "\n",
        "          start = input_len - overlap_window - 1\n",
        "\n",
        "          while start < len(label) - input_len + 1 :\n",
        "\n",
        "              data.append([start_token_id + input_id[start:start + input_len - 2] + end_token_id,\n",
        "                          outside_label + label[start:start + input_len - 2] + outside_label,\n",
        "                          calculate_attention_mask(input_len, input_len),\n",
        "                          0\n",
        "                          ])\n",
        "\n",
        "\n",
        "              start = start + input_len - overlap_window - 2\n",
        "\n",
        "              while label[start] != 'O':\n",
        "                start -= 1\n",
        "\n",
        "          data.append([start_token_id + input_id[start:len(label)],\n",
        "                      outside_label + label[start:len(label)],\n",
        "                      calculate_attention_mask(len(label) + 1 - start, input_len),\n",
        "                      1\n",
        "                      ])\n",
        "\n",
        "      else:\n",
        "\n",
        "          data.append([input_id,\n",
        "                      label,\n",
        "                      calculate_attention_mask(len(label) + 2, input_len),\n",
        "                      1\n",
        "                        ])\n",
        "\n",
        "\n",
        "  return data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import ast\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df):\n",
        "\n",
        "        if isinstance(df[\"labels\"].iloc[0], str):\n",
        "            df[\"labels\"] = df[\"labels\"].apply(lambda x: ast.literal_eval(x))\n",
        "\n",
        "        self.input_ids = torch.tensor(df[\"input_ids\"].tolist(), dtype=torch.long)\n",
        "        self.attention_mask = torch.tensor(df[\"attention_mask\"].tolist(), dtype=torch.long)\n",
        "        self.labels = torch.tensor(df[\"labels\"].tolist(), dtype=torch.long)\n",
        "        self.is_last_chunk = torch.tensor(df[\"is_last_chunk\"].astype(int).tolist(), dtype=torch.long)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"attention_mask\": self.attention_mask[idx],\n",
        "            \"labels\": self.labels[idx],\n",
        "            \"is_last_chunk\": self.is_last_chunk[idx]\n",
        "\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "YI1TTeWk1-2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "\n",
        "def map_labels_to_ids(labels, corrected_mapping):\n",
        "\n",
        "  return list(map(lambda x : corrected_mapping[x.replace(\"\\t\", \"\")], labels))\n",
        "\n",
        "\n",
        "def get_entity_mapping(file_path):\n",
        "\n",
        "  with open(file_path, 'r') as csvfile:\n",
        "      reader = csv.reader(csvfile)\n",
        "      keys = next(reader)\n",
        "      values = next(reader)\n",
        "      entity_mapping = dict(zip(keys, values))\n",
        "\n",
        "  corrected_mapping = {key: int(value) for key, value in entity_mapping.items()}\n",
        "\n",
        "  print(corrected_mapping)\n",
        "  return corrected_mapping\n",
        "\n",
        "\n",
        "\n",
        "def organize_data(data):\n",
        "\n",
        "  input_ids = [entry[0] for entry in data]\n",
        "  labels = [entry[1] for entry in data]\n",
        "  attention_mask = [entry[2] for entry in data]\n",
        "  is_last_chunk = [entry[3] for entry in data]\n",
        "\n",
        "  df = pd.DataFrame({\n",
        "      \"input_ids\": input_ids,\n",
        "      \"labels\": labels,\n",
        "      \"attention_mask\" : attention_mask,\n",
        "      \"is_last_chunk\" : is_last_chunk\n",
        "  })\n",
        "\n",
        "\n",
        "  file_path = '/content/drive/My Drive/entity_mapping.csv'\n",
        "  corrected_mapping = get_entity_mapping(file_path)\n",
        "\n",
        "\n",
        "  df[\"labels\"] = df[\"labels\"].apply(lambda x: map_labels_to_ids(x, corrected_mapping))\n",
        "  df[\"input_ids\"] = df[\"input_ids\"].apply(lambda x: x + ([0]*(512-len(x))))\n",
        "  df[\"labels\"] = df[\"labels\"].apply(lambda x: x + ([corrected_mapping['O']]*(512-len(x))))\n",
        "  df_copy = df.copy()\n",
        "\n",
        "  dataset = CustomDataset(df)\n",
        "  return dataset, df_copy, corrected_mapping\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m3bXOpFs2Fkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "\n",
        "def map_labels_to_ids(labels, corrected_mapping):\n",
        "\n",
        "  return list(map(lambda x : corrected_mapping[x.replace(\"\\t\", \"\")], labels))\n",
        "\n",
        "\n",
        "def get_entity_mapping(file_path):\n",
        "\n",
        "  with open(file_path, 'r') as csvfile:\n",
        "      reader = csv.reader(csvfile)\n",
        "      keys = next(reader)\n",
        "      values = next(reader)\n",
        "      entity_mapping = dict(zip(keys, values))\n",
        "\n",
        "  corrected_mapping = {key: int(value) for key, value in entity_mapping.items()}\n",
        "\n",
        "  print(corrected_mapping)\n",
        "  return corrected_mapping\n",
        "\n",
        "\n",
        "\n",
        "def organize_data(data):\n",
        "\n",
        "  input_ids = [entry[0] for entry in data]\n",
        "  labels = [entry[1] for entry in data]\n",
        "  attention_mask = [entry[2] for entry in data]\n",
        "  is_last_chunk = [entry[3] for entry in data]\n",
        "\n",
        "  df = pd.DataFrame({\n",
        "      \"input_ids\": input_ids,\n",
        "      \"labels\": labels,\n",
        "      \"attention_mask\" : attention_mask,\n",
        "      \"is_last_chunk\" : is_last_chunk\n",
        "  })\n",
        "\n",
        "\n",
        "  file_path = '/content/drive/My Drive/entity_mapping.csv'\n",
        "  corrected_mapping = get_entity_mapping(file_path)\n",
        "\n",
        "\n",
        "  df[\"labels\"] = df[\"labels\"].apply(lambda x: map_labels_to_ids(x, corrected_mapping))\n",
        "  df[\"input_ids\"] = df[\"input_ids\"].apply(lambda x: x + ([0]*(512-len(x))))\n",
        "  df[\"labels\"] = df[\"labels\"].apply(lambda x: x + ([corrected_mapping['O']]*(512-len(x))))\n",
        "  df_copy = df.copy()\n",
        "\n",
        "  dataset = CustomDataset(df)\n",
        "  return dataset, df_copy, corrected_mapping\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A6F4rMqbUxiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def data_preparation_pipeline(file_path, file_path_2):\n",
        "\n",
        "  # classes, annotations = load_test_data_from_json(file_path, file_path_2)\n",
        "  classes, annotations = read_data_from_json(file_path)\n",
        "\n",
        "  print(len(annotations))\n",
        "  correct_annotations = adjust_entity_boundaries(patterns, annotations)\n",
        "  cleaned_data = clean_data(correct_annotations)\n",
        "  aligned_data = align_data(cleaned_data)\n",
        "  annotated_bert, tokenizer = label_data(aligned_data)\n",
        "  data = prepare_dataset(annotated_bert)\n",
        "  dataset, df, corrected_mapping = organize_data(data)\n",
        "\n",
        "\n",
        "  return dataset, df, corrected_mapping\n",
        "\n",
        "\n",
        "json_file_path = '/content/drive/My Drive/20k_batch1 1.json'\n",
        "test_data_file_path = '/content/drive/My Drive/WD_(4129)/'\n",
        "test_data_file_path_2 = '/content/drive/My Drive/Completed annotations - 2.27.2023/'\n",
        "test_data_file_path_3 = '/content/drive/My Drive/7744_ner_v2_final.json'\n",
        "dataset, df, corrected_mapping = data_preparation_pipeline(test_data_file_path_3, test_data_file_path_2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pUTNqoq4U2Ab"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}