\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{multicol}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{hyperref}
\usepackage{listings}
\usepackage[a4paper, margin=1.5in]{geometry}
\title{FYP_final_27}
\author{dev.banthia21 }
\date{April 2025}


\begin{document}


\section{Abstract}

This project presents an intelligent, hybrid data extraction pipeline for real estate documents, with a focus on property deeds. The solution combines a machine learning-based approach—leveraging a BERT-based Named Entity Recognition (NER) model to identify complex fields—with a rule-based pattern matching system for extracting regular, well-structured fields. OCR preprocessing is handled by Azure OCR, and the system outputs structured data in CSV/Excel format without uncertainty estimation. \\

Evaluation on a test dataset of 400 deed documents—spanning warranty deeds, special warranty deeds, statutory warrants, and warranty deeds with vendor liens—showed over 75\% field-level accuracy across 20 target fields. A field prediction was counted as correct when it had at least 80\% overlap with the ground truth. \\

The layered architecture balances the generalisation capability of transformer-based models with the precision and interpretability of deterministic rules. While currently a prototype, the system shows strong potential for deployment, with scalability considerations for processing over 10,000 documents per day. If successful, it is expected to reduce manual data entry effort by at least 30\% in a workflow currently involving over 200 personnel. \\

Future work involves extending this approach to additional document types such as mortgages and validating performance on larger datasets. This work lays the foundation for scalable, high-accuracy automation in legal document processing. \\

\section{Introduction}

\subsection{Business motivations}

In the real estate industry, structured property data plays a pivotal role in enabling accurate decision-making, efficient operations, and the development of competitive digital platforms. With the growing digitisation of property transactions, companies across the sector—including data aggregators, real estate marketplaces, investment firms, and regulatory bodies—are increasingly focused on automating the extraction of key information from legal documents such as deeds, mortgages, and foreclosure agreements. \\

The demand for this data stems from several critical business needs. For real estate marketplaces, structured data underpins advanced search functionality, personalised recommendations, and transparent listings. Accurate information about property ownership, pricing, and legal status enhances user trust and improves the overall customer experience. Investors and financial institutions rely on such data for risk assessment, portfolio management, and underwriting decisions. Meanwhile, data solutions providers require scalable pipelines to aggregate property data from diverse sources and jurisdictions into unified, queryable databases. \\

In addition to supporting commercial platforms and analytics, accurate property data extraction also plays a critical role in legal compliance and title indexing. Title indexing is the process of systematically cataloguing property records to establish a clear chain of ownership, identify liens or encumbrances, and ensure the legal validity of property transactions. \\

County recorders, legal firms, and title insurance companies rely on accurate indexing of deed documents to facilitate property transfers and protect against disputes or fraud. Automating the extraction of relevant fields such as grantor and grantee names, legal descriptions, and parcel identifiers is therefore essential not only for operational efficiency but also for maintaining compliance with legal standards in real estate documentation. \\

\subsection{Challenges in Automated Data Extraction from Real Estate Documents}

Automating the extraction of structured data from real estate documents presents several critical challenges, both in terms of data quality and operational feasibility. \\

A primary obstacle lies in the nature of the source files—most documents are scanned images with poor resolution, skew, and inconsistent formatting. Document layouts vary significantly across counties, time periods, and document types, making reliable text extraction difficult. However, this challenge is largely mitigated by modern deep learning-based OCR tools such as Azure OCR, which are capable of handling noisy, irregular scans at a relatively low cost. \\

The second major challenge stems from the unstructured, legal nature of the content. Deeds and mortgages often contain domain-specific terminology, complex phrasing, and high variability in how key information is presented. This makes the consistent identification and extraction of fields like party names, addresses, or legal descriptions a non-trivial task. \\

With the advent of generative large language models (LLMs), it may seem natural to apply them to such problems. Indeed, LLMs demonstrate strong capabilities in understanding and summarising unstructured text. However, their use in high-volume legal data pipelines introduces serious concerns. Hallucination—the generation of text not present in the source—is an inherent risk, especially for long documents where key information may be scattered across multiple pages. LLMs also struggle with retaining factual accuracy across large contexts, such as 20-page mortgage documents. \\

Moreover, the cost of deploying LLMs at scale is prohibitive. Whether accessed via commercial APIs or hosted in-house, their compute requirements and token-based pricing far exceed acceptable industry thresholds. For context, the standard processing budget for a mortgage document is approximately 0.5 cents. Processing 10,000 multi-page documents daily using LLMs would significantly surpass this limit, making them unsuitable for core field extraction in production settings. \\

\subsection{Project Aim and Scope}

The primary aim of this project is to design an automated workflow capable of consistently achieving over 70\% accuracy in field identification across diverse types of real estate \textit{deeds}. The focus is strictly limited to deed documents; while similar in structure, \textit{mortgages} and other legal documents such as foreclosure records or invoices contain different fields and will require model retraining. However, the architecture and methodology developed here are intended to be extensible to these other document types once the pipeline demonstrates reliable performance on deeds. \\

The system is designed to extract the following fields from each deed:

\begin{multicols}{2}
\begin{itemize}
    \item Buyer name
    \item Seller name
    \item Buyer address
    \item Property address
    \item Assessor's parcel number
    \item Seller organization
    \item Buyer organization
    \item Recording date
    \item Document number
    \item Recording book number
    \item Recording page number
    \item Sales price amount
    \item Document date
    \item Signature date
    \item Effective date
    \item Document header
\end{itemize}
\end{multicols}

An annotated dataset of 7,700 deed documents is available for training and evaluation. However, this dataset includes ground truth labels for only the first seven fields. Due to the high cost and complexity of manual annotation, the remaining fields are to be captured using a \textit{rule-based pattern matching} approach. Once the rule-based extraction demonstrates high precision, its outputs can be used to generate pseudo-labeled data for training, effectively \textit{bootstrapping} the model to learn additional field types. \\

This approach offers a scalable and cost-efficient path forward, especially considering the anticipated daily volume of up to 10,000 documents. With continuous incoming data and iterative model retraining, the system is designed to improve over time and \textit{resist overfitting}, enabling it to generalise effectively to new layouts and unseen variations.

\subsection{Budget Constraints and Operational Impact}

Beyond the technical challenges, this project is shaped by stringent budgetary considerations. The current cost of processing each deed document is approximately \$0.10, amounting to an annual expenditure of nearly \$500,000 given the projected volume of over 10,000 documents per day. Relying solely on generative large language models (LLMs) for field extraction would far exceed this budget due to their high computational and token-based costs. Consequently, LLMs are excluded from the core extraction pipeline. \\

These financial constraints also influence the selection of OCR tools. It is essential to strike a balance between cost-efficiency and accuracy when choosing between commercial OCR services such as Azure OCR, Google Cloud Vision OCR, or potential open-source alternatives. \\

The potential impact of this project is significant. A successful implementation will not only improve operational efficiency by automating a labor-intensive process, but also enable the team to scale operations almost indefinitely. With in-house hardware resources as the only limiting factor, this workflow could support a major transformation in document processing capacity—positioning the organisation to win high-value clients and expand into new markets. \\

\section{Background}

\subsection{Real Estate Deeds: Structure and Content}

A real estate deed is a legal document used to transfer ownership of real property from one party to another. It is typically recorded by the county clerk or recorder's office and serves as the official, legally recognized evidence of a property transaction. The deed establishes who the grantor (seller) and grantee (buyer) are, and describes the property being transferred. \\

Deeds contain both essential and supplementary information. The essential fields generally include:
\begin{itemize}
\item Grantor (seller) name
\item Grantee (buyer) name
\item Property address
\item Legal description of the property
\item Assessor's Parcel Number (APN)
\item Recording date
\item Document number
\item Sales price (consideration amount), when applicable
\item Signatures and effective date
\item Party organizations (if applicable)
\end{itemize}

These fields are critical for indexing, title verification, and ensuring legal validity. Other content, such as boilerplate legal language, disclaimers, stamps, seals, and handwritten annotations, while legally necessary, is usually not relevant for structured data extraction. \\

This project involves deed documents sourced from dozens of counties across the United States. There is no standardized national format for deeds; even within a single county, formatting can vary depending on document age, software used, or recording procedures. \\

Format variations include:
\begin{itemize}
\item Field positioning: the same field may appear at different locations
\item Layout differences: single vs. multi-column, left-aligned vs. justified
\item Use of field labels: present in some documents, embedded in others
\item Variability in casing and punctuation
\item Presence or absence of structured sections or tables
\item Handwritten notes, stamps, and scanning artifacts
\end{itemize}

\subsection{Linguistic Variability and the Limitations of Rule-Based Extraction}

In addition to layout variability, deeds also exhibit significant linguistic variation. Important fields are often embedded within long, formal legal sentences using archaic language and inconsistent phrasing. This makes extraction using rigid rule-based methods unreliable.

\textbf{Example 1: Grantee (Buyer) Name}
\begin{quote}
\textit{"...the said party of the first part does hereby grant, bargain, sell, and convey unto John Doe, of the County of Norfolk and State of Massachusetts, the premises described as follows..."}
\end{quote}

Here, the grantee's name appears without a clear label and is surrounded by verbose phrasing. Rule-based systems would need complex, fragile patterns to reliably extract such information.

\textbf{Example 2: Consideration Amount (Sales Price)}
\begin{quote}
\textit{"In consideration of the sum of One Hundred Forty-Five Thousand and 00/100 Dollars (\$145,000.00), the receipt whereof is hereby acknowledged..."}
\end{quote}

This example demonstrates mixed numeric and textual representations of the sales amount within a broader legal sentence.

\textbf{Example 3: Document Date}
\begin{quote}
\textit{"...witnesseth that this indenture was made the fourteenth day of June, in the year of our Lord two thousand and twenty..."}
\end{quote}

Such expressions are non-standard and require contextual understanding to convert into structured formats.

\textbf{Example 4: Parcel Number (APN)}
\begin{quote}
\textit{"...being the same premises identified on the municipal tax maps as Lot 17, Block 42, also known as Parcel No. 220-064-017..."}
\end{quote}

The field "Parcel No." is embedded within geographic and historical context, not presented as a labeled value.

Due to this level of variability, rule-based extraction methods are limited in their ability to generalize across unseen formats. They require extensive handcrafted rules, are brittle to minor deviations in language, and do not scale well.

Machine learning-based techniques offer a more robust alternative, particularly for extracting fields such as buyer and seller names and addresses, which exhibit a high degree of variation in phrasing and structure. By learning patterns from data rather than relying on fixed templates, ML-based models are better equipped to handle diverse expressions of the same information and adapt to new, unseen document formats.


\subsection{Model Selection for Named Entity Recognition}

Named Entity Recognition (NER) is a token-level classification task that aims to identify and label key entities in unstructured text. In the context of real estate deeds, this includes fields such as names, addresses, dates, and parcel identifiers. Given the wide variation in how such entities are expressed, selecting an appropriate NER model is critical. \\

Traditional approaches to NER include statistical models like Conditional Random Fields (CRFs), which rely on handcrafted features and perform well when entity boundaries are predictable. However, they struggle to generalize to linguistically diverse inputs without extensive feature engineering. \\

Recurrent neural network-based models, such as Bidirectional LSTMs (BiLSTMs), improved performance by learning contextual representations of tokens in sequence. While BiLSTMs capture dependencies in both directions, they are inherently sequential, which limits their scalability and ability to model long-range dependencies effectively. \\

Transformer-based models have since become the state-of-the-art in NER, thanks to their ability to model global context using self-attention. These models process all tokens in parallel and weigh relationships between them dynamically, making them well-suited for complex, unstructured language like that found in legal documents. \\

Among transformer-based models, BERT (Bidirectional Encoder Representations from Transformers) has consistently achieved strong performance on NER benchmarks. BERT is pre-trained on large corpora using masked language modeling and is particularly effective at understanding the contextual meaning of words in varying syntactic and semantic environments. \\


One defining characteristic of BERT is its ability to process text bidirectionally. Unlike models that read left-to-right (e.g., GPT), bidirectional models consider both the left and right context of each token. For example, understanding whether "Jordan" refers to a person or a location often depends on the surrounding words on both sides. This bidirectionality is critical in NER, where the correct interpretation of a token frequently relies on full sentence context. \\

Given the diversity and inconsistency of legal language in deed documents, a transformer-based model with bidirectional attention offers a strong foundation for robust field extraction. The specific choice of architecture is discussed in detail in the following section. \\

A more detailed explanation of BERT's architecture and its adaptation to NER tasks is provided in Section~\ref{sec:bert}.

\subsection{Limitations of Using BERT Alone}

While transformer-based models such as BERT have demonstrated strong performance in named entity recognition (NER) tasks involving person names, organizations, and addresses, they are not inherently well-suited for extracting structured or numeric data fields. This is particularly relevant in the context of deed documents, which contain key fields like document numbers, book and page numbers, and various date formats. \\

\textbf{1. Lack of pretraining on numeric patterns:} BERT is pretrained on large, general-purpose corpora such as Wikipedia and BookCorpus. These datasets are rich in natural language but sparse in structured numerical expressions. As a result, BERT does not natively learn how to handle patterns such as ``2021-048293'' or ``Book 8472, Page 392''. \\

\textbf{2. Tokenisation issues:} BERT's WordPiece tokenizer tends to split numeric tokens into multiple subword units. For example, a document number like ``2021-048293'' may be tokenized into segments such as ``2021'', ``-``, ``048'', and ``293'', breaking up the semantic unity of the field and introducing noise during classification. \\

\textbf{3. No inductive bias for numeric structures:} Unlike rule-based systems, BERT has no built-in understanding that patterns like ``Book No. 8472'' or ``Parcel ID 220-064-017'' belong to a specific field type. It treats these tokens like any other arbitrary text, unless it has seen enough similar examples during fine-tuning. \\

\textbf{4. Attention dilution in long sentences:} In lengthy legal sentences, short numeric spans may receive disproportionately low attention weights. Without explicit positional guidance or layout cues, BERT may fail to assign sufficient importance to these spans. \\

\textbf{Implication:} For reliably extracting highly structured numeric fields, BERT alone may be insufficient. A hybrid approach that combines BERT-based entity recognition for unstructured fields with rule-based post-processing for deterministic patterns (e.g., dates, document numbers) offers a more robust and interpretable solution for end-to-end deed parsing.

\subsection{Regular Expressions for Field Extraction}

Regular expressions (regex) are a foundational tool for pattern matching, used to identify specific sequences of characters within text. In the context of field extraction from deed documents, regex facilitates the identification of structured tokens such as document numbers, dates, and parcel identifiers based on predefined textual patterns. \\

Regex enables the construction of both primitive and compound expressions. Primitive patterns match basic components—for example, \textbackslash d+ matches one or more digits, [-/] matches common separators, and \textbackslash b denotes word boundaries. These can be composed into more complex expressions to capture specific formats. For instance, a document number like "2021-048293" can be matched using \textbackslash d{4}-\textbackslash d{6}, while dates such as "06/14/2021" or "June 14, 2021" can be captured with tailored alternatives. \\

Regex is particularly effective when fields adhere to consistent structural rules, even within noisy or unstructured documents. It is computationally lightweight and easy to implement, making it a practical first-pass method for extracting well-defined numeric identifiers and standardized field types. \\

However, regular expressions have notable limitations. They are inherently brittle, requiring exact pattern matches, which makes them prone to failure when faced with inconsistent phrasing, formatting artifacts, or embedded legal language. Furthermore, regex lacks semantic understanding—it cannot infer meaning from context, which is critical when extracting ambiguous entities such as names or addresses that may vary in structure and location. Consequently, regex is best employed as a complementary technique within hybrid extraction systems, supporting more robust machine learning-based models that can generalize across variability.

\subsection{Hybrid Approach: Combining Regex and BERT for Field Extraction}

Given the diversity and complexity of real estate deed documents, a hybrid field extraction strategy is adopted that combines regular expressions for structured numeric fields with BERT-based models for context-dependent, variable text fields. This approach balances precision and flexibility, leveraging the strengths of both methodologies. \\

Regular expressions are used to extract fields that follow well-defined syntactic patterns, such as dates, document numbers, book/page references, and sales amounts. These patterns are carefully designed to account for a wide range of format variations observed across counties and document types. While regex is rigid and requires manual tuning, its reliability in identifying numeric fields with consistent delimiters makes it an effective tool for high-precision extraction. \\

In contrast, fields such as buyer and seller names, addresses, and legal entities are too variable in structure to be captured accurately by rule-based methods alone. For these fields, we employ a fine-tuned BERT-based model trained on annotated deed data. Unlike regex, BERT learns patterns from labelled examples and generalizes beyond seen formats. This is crucial when dealing with the linguistic variability and legal phrasing common in unstructured documents. \\

A key conceptual advantage of this hybrid setup is that regex-based extraction acts as an automatic annotator. Since thousands of deeds are processed daily, the successful pattern matching of numeric fields creates a growing pool of weakly labelled training data. This continuous stream of data can be used to retrain or fine-tune the BERT model on other fields, enabling the system to adapt and improve over time without full manual annotation. \\

While BERT may not excel at extracting exact numeric values, it often correctly identifies the surrounding context, helping localize the field of interest. In such cases, regex and BERT can be used in tandem—BERT handles the identification, while regex validates and extracts the precise value. This cooperative use of both tools enhances both robustness and accuracy, particularly in production environments where document variability is high and throughput requirements are large. \\

\section{Design}

\subsection{Word Embeddings: Motivation and Foundations}

Natural language is inherently symbolic and discrete, making it incompatible with numerical models unless converted into a continuous representation. Early approaches such as one-hot encoding represent words as binary vectors with dimensionality equal to the vocabulary size $V$. However, one-hot vectors suffer from two major drawbacks: they are sparse, and they encode no semantic similarity (e.g., the vectors for \texttt{dog} and \texttt{cat} are orthogonal despite their similarity).

To address this, we map each word $w_i$ to a dense, continuous embedding vector $\mathbf{e}_i \in \mathbb{R}^d$ such that semantically similar words are closer in Euclidean or cosine space. One popular method to learn such embeddings is the Continuous Bag of Words (CBOW) model, proposed as part of Word2Vec.

\subsubsection{CBOW Architecture}

The CBOW model is a shallow neural network that predicts a target word given its surrounding context words. Given a context window size $c$, the model takes the $2c$ words surrounding a center word $w_t$ and aims to predict $w_t$. The context words are encoded as one-hot vectors $\mathbf{x}_j \in \mathbb{R}^V$, which are projected into embeddings using a shared matrix $W \in \mathbb{R}^{V \times d}$:

\[
\mathbf{v}_j = W^\top \mathbf{x}_j
\]

The hidden layer computes the average of these context embeddings:

\[
\mathbf{v}_{\text{context}} = \frac{1}{2c} \sum_{j=1}^{2c} \mathbf{v}_j
\]

This hidden vector is then projected to a vocabulary-sized output using another weight matrix $W' \in \mathbb{R}^{d \times V}$:

\[
\mathbf{u} = W'^\top \mathbf{v}_{\text{context}}, \quad \hat{y}_k = \frac{e^{u_k}}{\sum_{j=1}^{V} e^{u_j}}
\]

The model is trained to minimize the negative log-likelihood of the true center word $w_t$:

\[
\mathcal{L}_{\text{CBOW}} = -\log \hat{y}_{w_t}
\]

\subsection{Limitations of Static Embeddings}

Although models like CBOW produce semantically meaningful embeddings, they are \emph{context-independent}—each word is assigned a single vector regardless of usage. For example, the word \texttt{bank} will have the same representation in both “river bank” and “bank account”.

Furthermore, the CBOW model treats the input as a bag-of-words and averages the context embeddings, completely discarding word order. This makes it incapable of capturing syntactic patterns and structural dependencies, which are often essential in legal and formal documents.

\subsection{Transformers: Contextual Embeddings through Self-Attention}

To address these limitations, transformer models were introduced by Vaswani et al.~(2017), providing a mechanism to learn context-sensitive embeddings without recurrence. In this architecture, every token can attend to all other tokens in the sequence using a mechanism called \emph{self-attention}, allowing it to dynamically weight the influence of different context words.

\subsubsection{Input Representations}

Given an input sequence of tokens $x_1, x_2, \dots, x_n$, each token is first mapped to an embedding $\mathbf{e}_i \in \mathbb{R}^d$. Since the transformer is permutation-invariant, positional information is added via a positional encoding vector $\mathbf{p}_i$, resulting in the input to the first layer:

\[
\mathbf{z}_i^{(0)} = \mathbf{e}_i + \mathbf{p}_i
\]

\subsection{Self-Attention Mechanism}

The key innovation in transformers is the self-attention mechanism, which computes an output vector for each position by aggregating information from all other positions, weighted by their relevance.

Each input vector $\mathbf{z}_i$ is linearly projected into three components:
\[
\mathbf{q}_i = \mathbf{z}_i W^Q, \quad \mathbf{k}_i = \mathbf{z}_i W^K, \quad \mathbf{v}_i = \mathbf{z}_i W^V
\]

where $W^Q, W^K, W^V \in \mathbb{R}^{d \times d_k}$ are learned weight matrices for queries, keys, and values respectively.

These can be interpreted as follows:
\begin{itemize}
  \item \textbf{Query} ($\mathbf{q}_i$): What the token is asking about other tokens.
  \item \textbf{Key} ($\mathbf{k}_j$): What a token offers in response.
  \item \textbf{Value} ($\mathbf{v}_j$): The actual content contributed if deemed relevant.
\end{itemize}

The attention weight between token $i$ and token $j$ is given by the scaled dot product between their query and key vectors:

\[
\alpha_{ij} = \frac{\exp\left(\frac{\mathbf{q}_i^\top \mathbf{k}_j}{\sqrt{d_k}}\right)}{\sum_{j'=1}^{n} \exp\left(\frac{\mathbf{q}_i^\top \mathbf{k}_{j'}}{\sqrt{d_k}}\right)}
\]

The output for token $i$ is then computed as a weighted sum of value vectors:

\[
\text{Attention}(\mathbf{q}_i, K, V) = \sum_{j=1}^{n} \alpha_{ij} \mathbf{v}_j
\]

In matrix form, this becomes:

\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
\]

\subsection{Stacked Encoder Layers and Contextualization}

A transformer encoder consists of multiple stacked layers, each with two main components:
\begin{enumerate}
  \item Multi-head self-attention: runs multiple attention operations in parallel with independent projections.
  \item Feed-forward network: applies two linear transformations with a non-linearity in between.
\end{enumerate}

Each layer also includes residual connections and layer normalization to facilitate gradient flow:

\[
\text{LayerNorm}(X + \text{Attention}(X)) \rightarrow \text{LayerNorm}(X + \text{FFN}(X))
\]

By stacking $L$ such encoder blocks, each token embedding becomes deeply contextualized—its final representation encodes rich semantic and syntactic information from the entire sequence.

These contextual embeddings can then be fed into a task-specific head, such as a linear classifier for token classification in named entity recognition.

\subsection{Fine-Tuning BERT for Named Entity Recognition}

To perform token-level field extraction from deed documents, we fine-tune BERT on a \emph{Named Entity Recognition} (NER) task. BERT's bidirectional transformer layers produce contextual embeddings for each input token, which are then classified into entity categories using a task-specific classification head.

\subsubsection{WordPiece Tokenization}

BERT uses a subword tokenization scheme known as \emph{WordPiece}. Instead of assigning embeddings to entire words, WordPiece breaks rare or unknown words into smaller units. For example:

\begin{center}
    \texttt{unaffordable} $\rightarrow$ [\texttt{un}, \texttt{\#\#afford}, \texttt{\#\#able}]
\end{center}

This allows the model to handle out-of-vocabulary words more gracefully. However, it also introduces a challenge for NER: entity labels are defined at the word level, while the input is tokenized at the subword level. To resolve this, labels are aligned only to the \emph{first subword} of each token. Subsequent subtokens are either ignored during loss computation or assigned a special label such as \texttt{X}.

\subsubsection{Entity Tagging Schemes}

We adopt the IOB tagging format to annotate entities:
\begin{itemize}
    \item \texttt{B-FIELD}: Beginning of a field (e.g., \texttt{B-BUYER\_NAME})
    \item \texttt{I-FIELD}: Inside a field span
    \item \texttt{O}: Outside any entity
\end{itemize}

Alternatively, more expressive schemes like BILOU (Begin, Inside, Last, Outside, Unit) can be used to better capture field boundaries, but IOB remains standard in many NER datasets and is compatible with BERT’s token-level classification setup.

\subsubsection{Token Classification Layer}

Let $\mathbf{h}_i \in \mathbb{R}^d$ be the contextual embedding of token $i$ output by BERT's final layer. This is passed through a linear classifier:

\[
\mathbf{y}_i = \text{softmax}(\mathbf{W} \mathbf{h}_i + \mathbf{b})
\]

where $\mathbf{W} \in \mathbb{R}^{C \times d}$ and $\mathbf{b} \in \mathbb{R}^C$, with $C$ being the number of entity classes (e.g., \texttt{B-BUYER\_NAME}, \texttt{I-BUYER\_NAME}, \texttt{O}, etc.). The predicted probability distribution over labels is compared against the ground truth labels using the **cross-entropy loss**:

\[
\mathcal{L}_{\text{NER}} = -\sum_{i=1}^{n} \sum_{c=1}^{C} y_{i,c}^{\text{true}} \log(y_{i,c}^{\text{pred}})
\]

During training, gradients are backpropagated through both the classification head and BERT encoder layers, updating all parameters via gradient descent. This fine-tuning allows the model to adapt its general linguistic knowledge to the domain-specific patterns in real estate documents.

\subsubsection{Training Setup}

The model is fine-tuned on a labelled dataset of 7,700 deed documents. The input is preprocessed into token-label pairs, and training is performed using the Adam optimizer with a learning rate warm-up schedule. Common hyperparameters include:
\begin{itemize}
    \item Batch size: 16 or 32
    \item Learning rate: $2 \times 10^{-5}$ to $5 \times 10^{-5}$
    \item Epochs: 3–5
    \item Max sequence length: typically 512 tokens
\end{itemize}

\subsubsection{Inference and Output Mapping}

At inference time, each token receives a predicted label. Subword labels are mapped back to words using a label aggregation strategy (e.g., taking the label of the first subtoken). Consecutive spans with \texttt{B-}/\texttt{I-} tags are grouped into field-level predictions and postprocessed into structured outputs.

Field-level accuracy is measured using an overlap criterion—typically, a prediction is considered correct if it matches the ground truth with at least 80\% character-level overlap.

\subsection{Labelled Dataset Format and Entity Alignment Pipeline}

To fine-tune the BERT-based Named Entity Recognition (NER) model for field extraction from deed documents, a robust multi-stage preprocessing pipeline was developed. The input consisted of raw OCR text and manually annotated character-level spans in a JSON format. This pipeline transformed the noisy, span-based annotations into a structure compatible with BERT’s token classification framework, ensuring that each token could be reliably assigned a label during training.

\subsubsection{Annotation Format}

Each record in the labelled dataset was stored as a JSON object containing:
\begin{itemize}
    \item \texttt{classes}: a list of field types such as \texttt{``BUYER NAME''}, \texttt{``SELLER NAME''}, \texttt{``PROPERTY ADDRESS''}, etc.
    \item \texttt{annotations}: a list of pairs \texttt{[text, \{entities\}]}, where \texttt{text} is the full OCR-extracted document and \texttt{entities} is a list of character-level spans in the form \texttt{[start\_char, end\_char, label]}.
\end{itemize}

For example:
\begin{verbatim}
["...unto John Doe, of the County...", {"entities": [[7, 15, "BUYER NAME"]]}]
\end{verbatim}

\subsubsection{Entity Cleaning and Boundary Correction}

Due to OCR artifacts, inconsistent annotation, and extraneous tokens, the following cleaning steps were applied:
\begin{itemize}
    \item Label typos (e.g., \texttt{``BUYER ADDDRESS''}) were normalized to standard forms.
    \item Only a curated set of core field types was retained.
    \item Artificial tokens such as \verb|<laysep@@##$$>| and escaped Unicode sequences (e.g., \verb|\u00a9|) were removed using regular expressions.
\end{itemize}

Cleaning altered the text content and could misalign character offsets. To fix this, entity positions were recalculated by computing the character count difference between the original and cleaned text up to each entity's start index. This offset correction preserved alignment between entity spans and the new cleaned text.

\subsubsection{Token Boundary Enforcement}

Even after cleaning, entity spans might not align with token boundaries. This misalignment arises because:
\begin{itemize}
    \item Entity start and end indices were manually annotated at the character level, independent of how BERT tokenizes text.
    \item BERT (and most NLP models) require entity spans to match token boundaries for correct label assignment.
\end{itemize}

To address this, the following procedure was used:
\begin{itemize}
    \item Tokens in the text were identified using regular expressions and stored as spans.
    \item Each entity span was checked for partial overlap with token boundaries.
    \item If a span intersected one or more tokens without fully covering them, its start and/or end indices were adjusted to snap to the nearest token boundary.
    \item If the resulting span was invalid (i.e., \texttt{start} $\geq$ \texttt{end}), it was discarded.
\end{itemize}

This clipping strategy reduced label noise caused by partially included tokens, which could otherwise confuse the model.

\subsubsection{Conversion to SpaCy-Compatible Format}

To ensure that annotated entity spans align correctly with token boundaries, each cleaned document was processed using \texttt{spaCy}, a widely used open-source natural language processing (NLP) library that provides efficient tokenization and linguistic annotations. \\

In \texttt{spaCy}, a processed document is represented by a \texttt{Doc} object. This object contains the original text along with tokenization metadata such as token indices and their corresponding character-level start and end positions. By converting each raw text string into a \texttt{Doc} object using \texttt{nlp.make\_doc(text)}, the pipeline gains access to this token structure, which is critical for aligning annotated character spans with tokens. \\

Each manually annotated entity span \texttt{[start, end, label]} was then passed to the method \texttt{doc.char\_span(...)}. This method attempts to convert a character-level span into a \texttt{Span} object, which represents a continuous sequence of tokens in the document. Importantly, \texttt{char\_span} includes an optional argument \texttt{alignment\_mode}, which determines how to handle spans that do not perfectly align with token boundaries. \\

In this pipeline, the mode \texttt{alignment\_mode="expand"} was used to address a common issue: annotated spans derived from OCR or human labelling often do not begin and end exactly at token boundaries. For example, a span might begin in the middle of a token (e.g., character 1 of the token ``John'') and end in the middle of another. If such a span were used directly during training, it would create ambiguity during token classification, as the model would not be able to cleanly assign a label to any individual token. \\

The \texttt{"expand"} mode resolves this by stretching the character span outward to the nearest full tokens. This ensures that entity spans are aligned cleanly with spaCy’s tokenization and are therefore compatible with token-level classification in transformer-based models. If a span could not be resolved to valid tokens even after expansion—such as if it landed entirely within whitespace or formatting artifacts—it was discarded to avoid injecting noise into the training set. \\

By leveraging spaCy’s alignment functionality in this way, the pipeline robustly transformed noisy, potentially misaligned entity annotations into token-aligned spans. This step was essential to bridge the gap between raw OCR-derived annotations and the requirements of models like BERT, which expect input labels to be defined at the token level.


\subsubsection{BERT Token Alignment and BIO Tagging}

After entity spans were aligned with token boundaries, the dataset was tokenized using HuggingFace’s \texttt{bert-base-uncased} tokenizer. Each document was passed through the tokenizer with the \texttt{return\_offsets\_mapping=True} argument, which returned both token identifiers and their corresponding character-level positions: \\

\begin{verbatim}
tokenizer(text, return_offsets_mapping=True)
\end{verbatim}

This produced:
\begin{itemize}
    \item \texttt{input\_ids}: a sequence of integer identifiers corresponding to subword tokens in BERT’s vocabulary.
    \item \texttt{offset\_mapping}: a list of \texttt{(start, end)} character positions indicating which part of the raw text each token corresponds to.
\end{itemize}

Each token was then assigned a label using the BIO (Begin–Inside–Outside) tagging scheme:
\begin{itemize}
    \item \texttt{B-FIELD}: indicates the beginning of an entity of type \texttt{FIELD}.
    \item \texttt{I-FIELD}: indicates that the token is inside (but not at the start of) an entity of type \texttt{FIELD}.
    \item \texttt{O}: indicates that the token does not belong to any entity.
\end{itemize}

The use of BIO tagging, rather than a simpler Inside–Outside (IO) scheme, is necessary to disambiguate adjacent entities of the same type. Without a \texttt{B-} tag, sequences like \texttt{I-BUYER NAME, I-BUYER NAME} could either mean a single multi-token entity or two consecutive entities of the same type, making decoding ambiguous. The BIO scheme resolves this by explicitly marking the boundary where a new entity begins. \\

While more expressive schemes such as BILOU (Begin, Inside, Last, Outside, Unit) offer finer granularity—especially for distinguishing between single-token and multi-token entities—BIO was chosen for its simplicity and widespread adoption in transformer-based NER models. It offers a good balance between expressive power and training stability, particularly given the relatively noisy and layout-influenced nature of OCR-derived text.

An illustrative example is shown below:
\begin{verbatim}
Entity span: [0, 8, "BUYER NAME"]
Text: "John Doe lives..."
Tokens: ["[CLS]", "john", "doe", ...]
Offsets: [(0,0), (0,4), (5,8), ...]

→ Labels: ["O", "B-BUYER NAME", "I-BUYER NAME", ...]
\end{verbatim}

Special tokens such as \texttt{[CLS]} and \texttt{[SEP]}, which have no corresponding character offsets (\texttt{(0,0)}), were excluded from the label assignment process.


\subsubsection{Final Output Format and Chunked Encoding Strategy}

Since many deed documents are longer than the 512-token limit imposed by BERT-based models, each tokenized document was split into overlapping fixed-length chunks. Specifically, a sliding window approach was used to create chunks of 512 tokens with a 50-token overlap between consecutive segments. \\

This chunking strategy addresses a core limitation of transformer models: they cannot attend to arbitrarily long sequences. However, naively truncating the document risks discarding important contextual information, particularly if an entity or its relevant keywords are split across chunk boundaries. The overlapping window mitigates this risk. By allowing 50 tokens of shared context between adjacent chunks, the model retains access to partial upstream context in subsequent windows, increasing the likelihood that fragmented entities can still be captured. \\

Each chunk was represented as a tuple of:
\begin{verbatim}
(input_ids, attention_mask, BIO_labels)
\end{verbatim}

\begin{itemize}
    \item \texttt{input\_ids}: A list of integers mapping each token in the chunk to an index in BERT’s vocabulary. These serve as the primary input to the model.
    
    \item \texttt{attention\_mask}: A binary vector of the same length as \texttt{input\_ids}, where each element indicates whether the corresponding token is real (1) or padding (0). This is essential for BERT's self-attention mechanism to ignore padded tokens when computing attention scores. Without the attention mask, BERT would treat padding as meaningful input, corrupting its contextual representations and degrading performance, especially during fine-tuning on short chunks or when batching variable-length samples.

    \item \texttt{BIO\_labels}: A list of string tags in the BIO format, aligned to each token in \texttt{input\_ids}, indicating the token’s entity role. These labels are used as supervision signals during training and are ignored (or masked) for special tokens such as \texttt{[CLS]}, \texttt{[SEP]}, or padding tokens.
\end{itemize}

All data preprocessing and model training were implemented in \texttt{PyTorch}, using the HuggingFace \texttt{Transformers} and \texttt{Datasets} libraries. PyTorch was selected over TensorFlow for several reasons:
\begin{itemize}
    \item It offers a more transparent and Pythonic programming model, which aligns well with the need for custom span adjustment, dynamic chunking, and debugging.
    \item PyTorch is the default backend for HuggingFace models and integrates seamlessly with its tokenizers, model classes, and training utilities.
    \item It provides better control over GPU memory usage and batch operations, which was particularly important given the variable-length nature of the tokenized OCR documents.
\end{itemize}

This final format — \texttt{(input\_ids, attention\_mask, BIO\_labels)} — formed the input to the token classification head of the BERT model. Each training sample enabled the model to learn fine-grained mappings from contextualized token embeddings to entity labels, thereby supporting accurate field-level extraction from unstructured, OCR-derived document text.

\subsection{Address Structure Preservation and Post-Processing}

While transformer-based models like BERT are highly effective at understanding linguistic context and identifying candidate address spans, they do not possess any inherent knowledge of the required structure or format of an address. Consequently, predictions often include partially formed addresses—omitting critical components such as ZIP codes, truncating street numbers (e.g., predicting ``\#\#32'' instead of ``1232''), or capturing incomplete state names. This compromises both the accuracy and downstream usability of the extracted fields. \\

To address this, a rule-based post-processing step was introduced to enforce structural consistency in the extracted address fields. This step was applied to both \texttt{BUYER ADDRESS} and \texttt{SELLER ADDRESS} fields after initial model inference. \\

\paragraph{Chunk Expansion and Pattern Matching.}
First, each extracted address span was matched against the full OCR-predicted text of the document. To ensure partial predictions were not missed, a search was performed using a regular expression that escaped the predicted substring and searched a 100-character window surrounding the match. Within this window, a more robust pattern-based search was then conducted to identify a fully formed address.

The address validation pattern was constructed as a combination of:
\begin{itemize}
    \item A street number: a 1–6 digit integer followed by whitespace.
    \item A flexible middle segment: up to 75 arbitrary characters to account for street name and directional markers.
    \item A valid US state: using full names and abbreviations, with optional punctuation (e.g., ``TX'', ``T.X.'', ``Texas'').
    \item An optional ZIP code: a five-digit code, optionally followed by a hyphenated four-digit suffix.
\end{itemize}

If this pattern matched a full address in the local context of the original prediction, the span was replaced with the complete address. Otherwise, the partially predicted string was retained as-is.

\paragraph{Regex-Based Noise Cleanup.}
Prior to matching, both the predicted address and OCR text were normalized by removing artificial symbols such as ``\#\#'' and collapsing whitespace around punctuation characters (e.g., turning ``1234 , Main St .'' into ``1234,Main St.''). This reduced the chances of tokenization artifacts interfering with pattern recognition.

\bigskip
\begin{algorithm}[H]
\caption{Normalize Extracted Address Using Context-Aware Regex}
\label{alg:address_normalization}
\DontPrintSemicolon

\KwIn{DataFrame \texttt{predictions\_df}, column name \texttt{col} (e.g., BUYER ADDRESS)}
\KwOut{Updated \texttt{predictions\_df[col]} with improved address predictions}

\vspace{2mm}
Define a regex \texttt{address\_pattern} composed of:\;
\Indp
Street number (1--6 digits) \\
+ Up to 75 arbitrary characters \\
+ Valid US state (abbreviation or full name) \\
+ Optional ZIP code
\Indm

\vspace{2mm}
\ForEach{row $i$ in \texttt{predictions\_df}}{
    
    Load the corresponding OCR text from file\;

    \vspace{1mm}
    Clean the OCR text by:\;
    \Indp Remove symbols like ``\#\#'' and normalize spaces around punctuation\;
    \Indm

    \vspace{1mm}
    \If{the predicted address in column \texttt{col} is not empty}{
    
        Clean the predicted address using the same rules\;
        Escape the address to build a search pattern\;

        \vspace{1mm}
        \If{pattern is found in the OCR text}{
            Extract a 100-character window around the match\;

            Search the window using \texttt{address\_pattern}\;

            \If{a valid full address is found}{
                Replace the predicted address with the full match\;
            }
        }
    }
}

\vspace{2mm}
\Return updated \texttt{predictions\_df}
\end{algorithm}
\bigskip


\paragraph{Why Learning Alone Is Not Sufficient.}
Although BERT can infer what constitutes an address based on training examples, it cannot enforce formatting constraints at inference time. There is no architectural mechanism to guarantee that a ZIP code, state abbreviation, or house number is present. The inclusion of rule-based post-processing thus complements the model by injecting structural priors into the pipeline — a form of hybrid learning and validation that improves precision without sacrificing recall.

\subsection{Post-Hoc Recovery of Missing Buyer Addresses}

While the BERT-based NER model was effective at identifying most address spans, a notable source of error involved missing \texttt{BUYER ADDRESS} fields. This occurred in cases where the address was not mentioned in close proximity to the buyer's name, but instead appeared in a separate section of the document — often introduced by template phrases such as ``Mail tax statements to:'' or ``Send notices to grantee at:''. \\

Because transformer models process tokens sequentially within a limited attention span (typically 512 tokens), they are prone to missing entities that are semantically related but **not spatially adjacent**. In such cases, the model correctly identifies the buyer name but fails to associate a later address as being relevant, especially when the address lacks an explicit label like ``BUYER ADDRESS''. \\

To recover such cases, a targeted rule-based procedure was implemented. The pipeline first identified all rows where the \texttt{BUYER ADDRESS} field was missing (i.e., \texttt{NaN} after model prediction). For each such row, the corresponding OCR-predicted text file was loaded for re-examination. \\

\paragraph{Name-Anchored Regex Search.}
The recovery strategy involved searching for address-like patterns that appear in the same vicinity as a buyer's name or organization name. The pipeline first constructed one or more regular expressions based on the predicted \texttt{BUYER NAME} or \texttt{BUYER ORG}. These patterns allowed for variability in spacing and punctuation, and supported structures such as:
\begin{itemize}
    \item ``John A. Smith''
    \item ``Smith, John A.''
    \item ``Smith and Johnson LLC''
\end{itemize}

Each name-derived anchor was then used to build a broader search expression that looked for:
\begin{itemize}
    \item A match to the name, followed by
    \item Up to 100 arbitrary characters, followed by
    \item A valid US state name or abbreviation, followed by
    \item A ZIP code (optionally in hyphenated 9-digit form)
\end{itemize}

\paragraph{Example Pattern.}
\begin{verbatim}
Pattern: John A. Smith.{0,100}(Texas|TX)\s+\d{5}(-\d{1,4})?
\end{verbatim}

This approach was robust to intervening text (e.g., introductory phrases or formatting tokens), and allowed the pipeline to capture address-like content that may have been completely missed by the model. \\

\paragraph{Normalization and Assignment.}
All matches were cleaned using whitespace and punctuation normalization to improve consistency with the rest of the dataset. The matched address was then written back to the \texttt{BUYER ADDRESS} column for the corresponding row in the prediction dataframe. 

\vspace{8mm} % or \bigskip


\begin{algorithm}[H]
\caption{Post-Hoc Buyer Address Recovery via Regex Anchored on Buyer Name or Organization}
\DontPrintSemicolon

\KwIn{Predictions dataframe with missing `BUYER ADDRESS` entries}
\KwOut{Updated `BUYER ADDRESS` field for recovered entries}

Initialize $recovered \gets 0$ \;
$missing\_indices \gets$ indices where BUYER ADDRESS is NaN \;

\ForEach{$i \in missing\_indices$}{

    Load OCR text from corresponding file path using \texttt{IMAGENAME} \;

    \eIf{BUYER NAME is not NaN}{
        Clean punctuation in buyer name\;
        $name\_patterns \gets$ BuildNameRegexes(buyer name)\;
    }{
        \eIf{BUYER ORG is not NaN}{
            Clean punctuation in buyer org\;
            $name\_patterns \gets$ \{buyer org\}\;
        }{
            \textbf{continue}\;
        }
    }

    $buyer\_addresses \gets [~]$ \tcp*{Empty list}

    \ForEach{$pattern \in name\_patterns$}{
        Construct regex pattern:
        \begin{itemize}
            \item pattern + up to 100 characters
            \item valid US state (full name or abbrev.)
            \item 5-digit or 9-digit ZIP code
        \end{itemize}
        
        \If{regex match found in OCR text}{
            Append matched address to $buyer\_addresses$\;
        }
    }

    \If{$buyer\_addresses$ is not empty}{
        Join all matched addresses into one string\;
        Assign to `BUYER ADDRESS` in predictions dataframe\;
        $recovered \gets recovered + 1$\;
    }
}

\Return updated predictions dataframe \;
\end{algorithm}

\vspace{8mm} 


\paragraph{Impact.}
This post-hoc recovery mechanism allowed the pipeline to reclaim a non-trivial number of missing buyer addresses, substantially improving recall for the most critical entity type in downstream use cases. It also demonstrates the utility of combining learning-based extraction with deterministic rules — especially when dealing with semi-structured legal documents where formatting conventions are predictable, but not always contiguous.

\subsection{Recording Metadata Extraction via Date-Time Anchoring}

Several important fields in deed documents—specifically the \textit{recording date}, \textit{document number}, \textit{book number}, and \textit{page number}—were not included in the initial annotated training data. Furthermore, as discussed in Section~\ref{sec:bert_limitations}, transformer-based models like BERT perform poorly when extracting structured numeric identifiers. This is due to both tokenization issues (e.g., fragmenting long numbers into subwords) and a lack of inductive bias for recognizing structured patterns like ``Book 8472'' or ``2024-003198''. 

As a result, a purely model-based approach was insufficient for reliably capturing these fields. Instead, we employ a \textbf{deterministic, pattern-based strategy} where the \textit{recording date} is used as a robust anchor point for extracting all associated recording metadata.

\subsubsection{Motivation for Anchoring on Recording Date}

While other dates such as the document date or signature date may appear multiple times and in ambiguous contexts, the recording date is uniquely identifiable: it is the \textbf{only date in the document followed by a time stamp}. This consistent structural pattern makes it a highly reliable anchor for locating nearby metadata.

Typical recording date formats include:

\begin{itemize}
    \item \texttt{April 3, 2023 at 11:24 AM}
    \item \texttt{04/03/2023 11:24:15}
    \item \texttt{the 3rd day of April, 2023 - 14:02}
\end{itemize}

This makes the recording date one of the few fields that can be accurately identified through syntactic structure alone, without relying on surrounding labels or semantics.

\subsubsection{Regex Construction for Robust Date-Time Matching}

To detect recording dates with high recall and precision, a composite regular expression was constructed to match a wide range of formats. The final pattern is composed of:

\begin{itemize}
    \item \textbf{Date component}: ISO, US, and formal legal phrasings.
    \item \textbf{Time component}: 24-hour and 12-hour formats, with or without seconds and AM/PM.
    \item \textbf{Flexible separator}: keywords like ``at'', hyphens, commas, or up to 20 arbitrary characters.
\end{itemize}

In Python-like notation, the pattern is structured as:

\texttt{date\_time\_pattern = (date\_pattern)(separator)(time\_pattern)}

This pattern is matched using \texttt{re.finditer} across the full OCR text. The first plausible match—typically near the top of the document—is selected as the recording date.

\subsubsection{Using Recording Date as an Anchor}

A crucial benefit of using the recording date-time as an anchor is that it enables \textbf{focused, high-precision extraction} of nearby recording metadata. Fields such as the document number, book number, and page number frequently appear in deed documents—but not always in relevant contexts. For example, similar numeric patterns often occur in stamp annotations, boilerplate text, or historical references to other documents. Without an anchor, searching the full document for patterns like \texttt{Book 8472} or \texttt{Doc\# 2023-001512} yields a large number of spurious matches, leading to high false positive rates. \\

By contrast, the recording date-time block occurs only once and follows a distinctive format that is highly consistent across jurisdictions: a date immediately followed by a time. Once this match is found, it serves as a reliable reference point for locating nearby fields that are semantically related. \\ 

Instead of scanning the entire document, the system performs a \textit{localized search} around the anchor—narrowing the context window to a small region. This drastically reduces the search space, minimizes false detections, and increases the likelihood that extracted values truly correspond to the deed's official recording metadata. \\

This anchoring strategy is not only more accurate but also more computationally efficient, making it especially well-suited to high-volume automated pipelines.

\subsection{Document Number Extraction Using Anchored Pattern Matching}

The document number is a critical field in deed documents, used for indexing, retrieval, and referencing official records in county-level systems. However, this field exhibits considerable variation in format and position across jurisdictions, and was not part of the initial annotated training data. Given BERT’s limitations in handling structured numeric patterns—especially in the absence of explicit labels—a dedicated, regex-based extraction pipeline was developed.

\subsubsection{Regex Strategy and Contextual Scoping}

The extraction process begins by using the previously identified \textbf{recording date-time} as a positional anchor (see Section~\ref{sec:recording_anchor}). Since document number matches appear frequently in the text—both in relevant and irrelevant contexts—anchoring is essential to localize the search region and minimize false positives.

To enforce structure, the full OCR text is segmented into \textit{blocks} using layout separators (e.g., custom tokens like \texttt{<laysep@@\#\#\$\$>}) as boundaries. The block that contains the recording date-time is identified, and the system then searches the immediately preceding and following blocks for potential document number candidates.

\subsubsection{Document Number Format}

Valid document numbers typically follow one of three formats:

\begin{itemize}
    \item A single long number: e.g., \texttt{2024000321}
    \item A compound identifier: e.g., \texttt{2024-00054321}
    \item An underscore-based variant: e.g., \texttt{2024\_00054321}
\end{itemize}

These are captured using the following generalized pattern:

\begin{center}
\texttt{(optional prefix) [4–15 digits] [optional dash/underscore] [4–15 digits]}
\end{center}

Whitespace and optional delimiters are accounted for to make the pattern resilient to OCR artifacts.

\subsubsection{Filtering Invalid Matches}

Not all numeric matches near the anchor are valid. Many deeds contain boilerplate segments or embedded key-value pairs with unrelated identifiers. For instance:

\begin{itemize}
    \item \texttt{Order No. 23-27216}
    \item \texttt{Tax Account No. 0117513}
\end{itemize}

To eliminate such false positives, the system uses a second-stage heuristic filter. It scans for document-number-like patterns that appear within \textit{double-key structures} (e.g., two adjacent keywords such as \texttt{Map No.} or \texttt{Order Number}) but do not contain trusted prefixes like ``document number'' or ``instrument ID''. If such a match lacks an explicit label, it is flagged as invalid and excluded.

\subsubsection{Explicit Label Matching}

In addition to context-based detection, a separate pass is performed to locate explicitly labeled document numbers. These use trusted lexical patterns, such as:

\begin{itemize}
    \item \texttt{Document Number: 2024-001231}
    \item \texttt{Instrument ID - 2024\_001231}
    \item \texttt{Recording No. 2024001231}
\end{itemize}

A flexible regex is used to detect these structures, combining known prefixes (e.g., \texttt{doc}, \texttt{instrument}, \texttt{AFN}, \texttt{recording}) with suffixes (e.g., \texttt{number}, \texttt{id}, \texttt{no.}, \texttt{\#}) and separators (colon, dash, space). The matching value is then cleaned and parsed as a candidate.

\subsubsection{Final Selection and Normalization}

After extracting both anchor-adjacent and explicitly labeled matches, the candidates are normalized to eliminate superficial formatting differences. For instance, \texttt{2024-000321} and \texttt{2024-321} are considered equivalent after stripping leading zeros in the suffix component.

The final value is selected as follows:

\begin{itemize}
    \item If one candidate occurs multiple times (i.e., mode frequency > 1), it is selected.
    \item If not, the candidate that structurally aligns with both the anchor-adjacent set and the explicitly labeled set is chosen.
    \item If no such match exists, a fallback strategy returns all unique candidates, prioritizing those with the most complete structure.
\end{itemize}

\subsubsection{Example Output}

Given a recording date-time anchor and the surrounding blocks, the following document number block:

\begin{quote}
\texttt{<laysep@@\#\#\$\$>\\2024-000621\\<laysep@@\#\#\$\$>}
\end{quote}

would be correctly extracted and returned as:

\begin{quote}
\texttt{2024-000621}
\end{quote}

\subsubsection{Summary}

This pipeline combines structural pattern matching, layout-aware scoping, and label-based heuristics to reliably extract document numbers in the absence of explicit annotations or model support. By anchoring the extraction around the recording timestamp and applying aggressive filtering, the system achieves high precision while remaining generalizable across diverse jurisdictions and deed formats.

\subsection{Book and Page Number Extraction via Contextual Filtering}

Deed documents often include a \textit{book number} and \textit{page number} pair as part of their official recording information. These identifiers serve as physical or digital references to where the document is stored in the county recorder’s system. However, the placement, formatting, and structure of these fields vary significantly across jurisdictions and time periods, making their extraction non-trivial.

\subsubsection{Variability in Format and Language}

Book and page fields appear in many surface forms, such as:

\begin{itemize}
    \item \texttt{Book 8472, Page 231}
    \item \texttt{Bk No. 123 Vol. 9}
    \item \texttt{Vol: 49 and Pg: 200}
\end{itemize}

To accommodate this variation, we define the following elements:

\begin{itemize}
    \item \textbf{Field labels:} Keywords such as \texttt{book}, \texttt{bk.}, \texttt{volume}, \texttt{vol.} for the book; and \texttt{page}, \texttt{pg.}, \texttt{p.} for the page.
    \item \textbf{Suffixes:} Optional terms like \texttt{number}, \texttt{num.}, or \texttt{no.}
    \item \textbf{Separators:} Variants including colons, dashes, hyphenated phrases, commas, conjunctions like ``and'', or freeform spacing.
    \item \textbf{Numeric pattern:} One or two numbers, often written as a single value (e.g., \texttt{8472}) or as a range (e.g., \texttt{231--234}).
\end{itemize}

The resulting composite regular expression is designed to match pairs such as:

\begin{quote}
\texttt{Book No. 8472 -- Page 231}, \quad \texttt{Bk: 1134 and Pg: 92}, \quad \texttt{Volume 22 Pages 103-104}
\end{quote}

\subsubsection{Contextual Filtering via Recording Date Anchor}

Given that similar book/page references may appear elsewhere in the document (e.g., citations to other deeds), simply applying a global regex often yields false positives. To reduce this noise, we adopt the same anchoring strategy used for document number extraction: once the recording date-time is detected, a \textit{context window} is defined spanning the blocks immediately before and after the block containing the match. \\

These blocks are defined using a document segmentation token (\texttt{<laysep@@\#\#\$\$>}), which allows the text to be split into logical units. The book/page extraction then operates only within this three-block region. \\

Only matches whose spans fall entirely within this context window are retained. This filtering mechanism effectively narrows the search to the header region or recorder section, where the true book/page fields are most likely to appear.

\subsubsection{Disambiguation and Range Handling}

When multiple valid book-page pairs are detected in the filtered region, a post-processing function is applied to select the most reliable pair:

\begin{itemize}
    \item If all book numbers are identical and only one unique page number is found, that pair is returned.
    \item If the page numbers differ but contain no ranges (e.g., \texttt{231}, \texttt{233}), the page field is returned as a range spanning the min and max values (e.g., \texttt{231--233}).
    \item If one of the entries already encodes a page range (e.g., \texttt{231--234}), it is directly returned.
\end{itemize}

This approach prioritizes precision while still accommodating common edge cases such as scanned multi-page deeds, disjoint metadata fields, or over-annotated headers.

\subsubsection{Summary}

By combining flexible regular expressions with spatial anchoring and context-aware disambiguation, the system reliably extracts book and page identifiers across a wide variety of layouts. This design significantly reduces false positives without sacrificing recall, and ensures compatibility with downstream indexing and validation processes.



\end{document}
