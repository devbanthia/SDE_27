{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-OimWJVKOrU"
      },
      "outputs": [],
      "source": [
        "\n",
        "missing_buyer_addr = predictions_df[predictions_df['BUYER ADDRESS'].isna()].index.tolist()\n",
        "print(len(missing_buyer_addr))\n",
        "\n",
        "state = r\"\\b(?:alabama|al|alaska|ak|arizona|az|arkansas|ar|california|ca|colorado|co|connecticut|ct|delaware|de|florida|fl|georgia|ga|hawaii|hi|idaho|id|illinois|il|indiana|in|iowa|ia|kansas|ks|kentucky|ky|louisiana|la|maine|me|maryland|md|massachusetts|ma|michigan|mi|minnesota|mn|mississippi|ms|missouri|mo|montana|mt|nebraska|ne|nevada|nv|new hampshire|nh|new jersey|nj|new mexico|nm|new york|ny|north carolina|nc|north dakota|nd|ohio|oh|oklahoma|ok|oregon|or|pennsylvania|pa|rhode island|ri|south carolina|sc|south dakota|sd|tennessee|tn|texas|tx|utah|ut|vermont|vt|virginia|va|washington|wa|west virginia|wv|wisconsin|wi|wyoming|wy)\\b\"\n",
        "zip_code = r\"\\b(?:\\d{5}(?:\\s*-\\s*\\d{1,4})?)\\b\"\n",
        "\n",
        "def build_name_regexes(buyer_name_raw):\n",
        "\n",
        "    names = re.split(r'\\s*,\\s*|\\s+and\\s+', buyer_name_raw, flags=re.IGNORECASE)\n",
        "    regex_patterns = []\n",
        "\n",
        "    for name in names:\n",
        "        name = name.strip()\n",
        "\n",
        "        name_parts = name.split()\n",
        "\n",
        "        if not name_parts:\n",
        "            continue\n",
        "\n",
        "        pattern = re.escape(name_parts[0])\n",
        "\n",
        "        for middle in name_parts[1:-1]:\n",
        "            pattern += r'\\s+(?:' + re.escape(middle) + r'|' + re.escape(middle[0]) + r'\\.?)?'\n",
        "\n",
        "        if len(name_parts) > 1:\n",
        "            pattern += r'\\s+' + re.escape(name_parts[-1])\n",
        "\n",
        "        regex_patterns.append(pattern)\n",
        "\n",
        "    return regex_patterns\n",
        "\n",
        "\n",
        "\n",
        "recovered = 0\n",
        "for i in missing_buyer_addr:\n",
        "\n",
        "\n",
        "  file_name = predictions_df.loc[i, 'IMAGENAME']\n",
        "  print(file_name)\n",
        "  folder_path = '/content/drive/MyDrive/ocr_output_2000/'\n",
        "  full_path = f\"{folder_path}{file_name}\"\n",
        "\n",
        "  with open(full_path, 'r', encoding='utf-8') as f:\n",
        "    doc_text = f.read()\n",
        "\n",
        "\n",
        "  buyer_name = predictions_df.loc[i, 'BUYER NAME']\n",
        "  buyer_org = predictions_df.loc[i, 'BUYER ORG']\n",
        "\n",
        "  if pd.notna(buyer_name):\n",
        "\n",
        "    buyer_name = re.sub(r'\\s+([.,])', r'\\1', buyer_name)\n",
        "    buyer_name_patterns = build_name_regexes(buyer_name)\n",
        "\n",
        "  elif pd.notna(buyer_org):\n",
        "    buyer_org = re.sub(r'\\s+([.,])', r'\\1', buyer_org)\n",
        "    buyer_name_patterns = [rf\"{buyer_org}\"]\n",
        "\n",
        "  else:\n",
        "    continue\n",
        "\n",
        "  buyer_addrs = []\n",
        "  for buyer_name_pattern in buyer_name_patterns:\n",
        "\n",
        "    buyer_addr_pattern = rf\"{buyer_name_pattern}.{{0,100}}{state}\\s+{zip_code}\"\n",
        "    buyer_addr = re.search(buyer_addr_pattern, doc_text, re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "    if buyer_addr is not None:\n",
        "      buyer_addrs.append(buyer_addr.group())\n",
        "\n",
        "  if len(buyer_addrs) > 0:\n",
        "    predictions_df.at[i, 'BUYER ADDRESS'] = ' '.join(buyer_addrs)\n",
        "    recovered += 1\n",
        "    print(buyer_addrs)\n",
        "    print(\"\\n\")\n",
        "\n",
        "print(recovered)\n",
        "str_columns = [\"APN\",\"consideration\",\"recording_date\",\"doc_date\",\"signature_date\",\"effective_date\",\"book_num\",\"page_num\",\"document_number\"]\n",
        "predictions_df[str_columns] = predictions_df[str_columns].astype(str)\n",
        "predictions_df.to_excel(\"2000_docs_predictions.xlsx\", index=False, engine=\"openpyxl\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def all_matches(pattern, text, address_pattern):\n",
        "  text = text.lower()\n",
        "  matches = re.finditer(pattern, text)\n",
        "  for m in matches:\n",
        "    start, end = m.span()\n",
        "    expanded_text = text[start-50: end+50]\n",
        "    full_addr = re.search(address_pattern, expanded_text, re.DOTALL)\n",
        "    if full_addr is not None:\n",
        "      return full_addr.group()\n",
        "\n",
        "  return None\n",
        "\n",
        "\n",
        "def normalize_address(combined_df, col):\n",
        "\n",
        "  street_number = r\"(?:\\b\\d{1,6}\\s+)\"\n",
        "  middle_address = r\"(?:.){0,75}\"\n",
        "  state = r\"\\b(?:alabama|a\\.?l\\.?|alaska|a\\.?k\\.?|arizona|a\\.?z\\.?|arkansas|a\\.?r\\.?|california|c\\.?a\\.?|colorado|c\\.?o\\.?|connecticut|c\\.?t\\.?|delaware|d\\.?e\\.?|florida|f\\.?l\\.?|georgia|g\\.?a\\.?|hawaii|h\\.?i\\.?|idaho|i\\.?d\\.?|illinois|i\\.?l\\.?|indiana|i\\.?n\\.?|iowa|i\\.?a\\.?|kansas|k\\.?s\\.?|kentucky|k\\.?y\\.?|louisiana|l\\.?a\\.?|maine|m\\.?e\\.?|maryland|m\\.?d\\.?|massachusetts|m\\.?a\\.?|michigan|m\\.?i\\.?|minnesota|m\\.?n\\.?|mississippi|m\\.?s\\.?|missouri|m\\.?o\\.?|montana|m\\.?t\\.?|nebraska|n\\.?e\\.?|nevada|n\\.?v\\.?|new hampshire|n\\.?h\\.?|new jersey|n\\.?j\\.?|new mexico|n\\.?m\\.?|new york|n\\.?y\\.?|north carolina|n\\.?c\\.?|north dakota|n\\.?d\\.?|ohio|o\\.?h\\.?|oklahoma|o\\.?k\\.?|oregon|o\\.?r\\.?|pennsylvania|p\\.?a\\.?|rhode island|r\\.?i\\.?|south carolina|s\\.?c\\.?|south dakota|s\\.?d\\.?|tennessee|t\\.?n\\.?|texas|t\\.?x\\.?|utah|u\\.?t\\.?|vermont|v\\.?t\\.?|virginia|v\\.?a\\.?|washington|w\\.?a\\.?|west virginia|w\\.?v\\.?|wisconsin|w\\.?i\\.?|wyoming|w\\.?y\\.?)\\b\"\n",
        "  zip_code = r\"(?:\\d{5}(?:\\s*-\\s*\\d{1,4})?)\\b\"\n",
        "\n",
        "  address_pattern = rf\"{street_number}{middle_address}{state}\\s*(?:,)?\\s*(?:{zip_code})?\"\n",
        "\n",
        "  folder_path = \"/content/drive/MyDrive/ocr_output_2000/\"\n",
        "\n",
        "  for i in range(len(combined_df)):\n",
        "\n",
        "    filename = combined_df.loc[i, 'IMAGENAME'] + \".txt\"\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "      text = file.read()\n",
        "\n",
        "    text = re.sub(r'\\s*([,.:#\\-])\\s*', r'\\1', text)\n",
        "    addr = combined_df.loc[i, col]\n",
        "    if pd.notna(addr):\n",
        "\n",
        "      addr = re.sub(r'\\s*([,.:#\\-])\\s*', r'\\1', addr)\n",
        "      addr = addr.replace(\"##\", \"\")\n",
        "      pattern = re.escape(addr)\n",
        "      m = re.search(pattern, text.lower())\n",
        "\n",
        "      if m:\n",
        "\n",
        "        start, end = m.span()\n",
        "        full_addr = all_matches(pattern, text, address_pattern)\n",
        "        combined_df.loc[i, col] = full_addr\n",
        "\n",
        "      else:\n",
        "        combined_df.loc[i, col] = addr\n",
        "\n",
        "\n",
        "  return combined_df\n",
        "\n",
        "col = \"BUYER ADDRESS\"\n",
        "combined_df = normalize_address(combined_df, col)\n",
        "print(combined_df[col])\n"
      ],
      "metadata": {
        "id": "hgfJplwLKfG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y libsnappy-dev autoconf automake libtool curl\n",
        "\n",
        "!git clone https://github.com/openvenues/libpostal\n",
        "%cd libpostal\n",
        "!./bootstrap.sh\n",
        "!./configure --datadir=/usr/local/share/libpostal\n",
        "!make -j4\n",
        "!make install\n",
        "!ldconfig\n",
        "\n"
      ],
      "metadata": {
        "id": "YOLfq1LYzv5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install postal\n",
        "from postal.parser import parse_address"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "CCAk-H-IzwgO",
        "outputId": "536e61a0-7e73-411f-9479-f9a99aef35b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting postal\n",
            "  Downloading postal-1.1.10.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from postal) (1.17.0)\n",
            "Building wheels for collected packages: postal\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for postal (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for postal\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for postal\n",
            "Failed to build postal\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (postal)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'postal'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-55118a9c8826>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install postal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpostal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_address\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'postal'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl\n",
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/pred.xlsx\"\n",
        "df = pd.read_excel(file_path, engine='openpyxl')"
      ],
      "metadata": {
        "id": "zpLVwJAIz5I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "property_address_cols = [\n",
        "    'PropertyHouseNumber', 'PropertyHouseNumberExt',\n",
        "    'PropertyStreetPreDirectional', 'PropertyStreetName', 'PropertyStreetSuffix',\n",
        "    'PropertyStreetPostDirectional', 'PropertyBuildingNumber',\n",
        "    'PropertyUnitDesignator', 'PropertyUnit', 'PropertyCity',\n",
        "    'PropertyState', 'PropertyZip', 'PropertyZip4'\n",
        "]"
      ],
      "metadata": {
        "id": "OBoJWhRbz-k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "from postal.parser import parse_address\n",
        "import re\n",
        "\n",
        "valid_entities = ['house_number', 'road', 'unit', 'city', 'state', 'postcode']\n",
        "\n",
        "import re\n",
        "\n",
        "def extract_directionals(street_name):\n",
        "\n",
        "    cleaned = re.sub(r'[\\.\\-]', ' ', street_name).upper()\n",
        "    tokens = cleaned.strip().split()\n",
        "\n",
        "    full_to_abbrev = {\n",
        "        'NORTH': 'N', 'SOUTH': 'S', 'EAST': 'E', 'WEST': 'W',\n",
        "        'NORTHEAST': 'NE', 'NORTHWEST': 'NW',\n",
        "        'SOUTHEAST': 'SE', 'SOUTHWEST': 'SW',\n",
        "        'N': 'N', 'S': 'S', 'E': 'E', 'W': 'W',\n",
        "        'NE': 'NE', 'NW': 'NW', 'SE': 'SE', 'SW': 'SW',\n",
        "        'N E': 'NE', 'N W': 'NW', 'S E': 'SE', 'S W': 'SW',\n",
        "        'NORTH EAST': 'NE', 'NORTH WEST': 'NW', 'SOUTH EAST': 'SE', 'SOUTH WEST': 'SW'\n",
        "    }\n",
        "\n",
        "    pre = post = \"\"\n",
        "    i = 0\n",
        "\n",
        "    for span in [2, 1]:\n",
        "      pre_candidate = \" \".join(tokens[:span])\n",
        "      if pre_candidate in full_to_abbrev:\n",
        "        pre = full_to_abbrev[pre_candidate]\n",
        "        tokens = tokens[span:]\n",
        "        break\n",
        "\n",
        "    for span in [2, 1]:\n",
        "      post_candidate = \" \".join(tokens[-span:])\n",
        "      if post_candidate in full_to_abbrev:\n",
        "        post = full_to_abbrev[post_candidate]\n",
        "        tokens = tokens[:-span]\n",
        "        break\n",
        "\n",
        "    core = \" \".join(tokens)\n",
        "    return pre, core.strip(\", \"), post\n",
        "\n",
        "\n",
        "\n",
        "def clean_address(raw_text, addr):\n",
        "    valid_entities = ['house_number', 'road', 'unit', 'city', 'state', 'postcode']\n",
        "\n",
        "    if 'house_number' not in addr or not addr['house_number']:\n",
        "        return raw_text\n",
        "\n",
        "    house_number = addr['house_number'][-1]\n",
        "    house_number_match = re.search(re.escape(house_number), raw_text, re.IGNORECASE)\n",
        "    if not house_number_match:\n",
        "        return raw_text\n",
        "\n",
        "    lower_bound = house_number_match.span()[1]\n",
        "\n",
        "    for succeeding_field_index in range(2, len(valid_entities)):\n",
        "        field = valid_entities[succeeding_field_index]\n",
        "        if field in addr and addr[field]:\n",
        "            break\n",
        "    else:\n",
        "        return raw_text  # No valid succeeding field found\n",
        "\n",
        "\n",
        "    succeeding_field = valid_entities[succeeding_field_index]\n",
        "    succeeding_value = addr[succeeding_field][0]\n",
        "    succeeding_match = re.search(re.escape(succeeding_value), raw_text, re.IGNORECASE)\n",
        "\n",
        "    if not succeeding_match:\n",
        "        return raw_text\n",
        "\n",
        "    upper_bound = succeeding_match.span()[0]\n",
        "\n",
        "    road = None\n",
        "    if 'road' in addr:\n",
        "      for r in addr['road']:\n",
        "        road_match = re.search(re.escape(r), raw_text, re.IGNORECASE)\n",
        "        if road_match:\n",
        "          start, end = road_match.span()\n",
        "          if lower_bound <= start <= end <= upper_bound:\n",
        "            road = r\n",
        "            break\n",
        "\n",
        "    clean_addr = house_number + \" \"\n",
        "\n",
        "\n",
        "    for f in valid_entities[1:]:\n",
        "\n",
        "      if f == \"road\" and road:\n",
        "        clean_addr += \"\".join(road)\n",
        "\n",
        "      elif f in addr and addr[f]:\n",
        "          clean_addr += f\", {addr[f][0]}\"\n",
        "\n",
        "\n",
        "    return clean_addr\n",
        "\n",
        "\n",
        "\n",
        "def normalize_directionals(label_list):\n",
        "  new_list = []\n",
        "\n",
        "  for value, label in label_list:\n",
        "    if label == 'road':\n",
        "      pre, core, post = extract_directionals(value)\n",
        "      if pre:\n",
        "        new_list.append((pre.lower(), 'directional_prefix'))\n",
        "\n",
        "      new_list.append((core.lower(), 'road'))\n",
        "      if post:\n",
        "        new_list.append((post.lower(), 'directional_suffix'))\n",
        "\n",
        "    elif label == 'postcode':\n",
        "\n",
        "      zip_parts = value.strip().split('-')\n",
        "      new_list.append((zip_parts[0], 'postcode'))\n",
        "      if len(zip_parts) > 1:\n",
        "        new_list.append((zip_parts[1], 'postcode_ext'))\n",
        "    else:\n",
        "      new_list.append((value, label))\n",
        "\n",
        "  return new_list\n",
        "\n",
        "\n",
        "\n",
        "def parse_with_libpostal(raw_text):\n",
        "\n",
        "    result = {'house_number':[], 'road':[], 'unit':[], 'city':[], 'state':[], 'postcode':[] }\n",
        "    parsed = parse_address(raw_text)\n",
        "\n",
        "    for value, field in parsed:\n",
        "      if field in valid_entities:\n",
        "        result[field] = result[field] + [value]\n",
        "\n",
        "    clean_addr= clean_address(raw_text, result)\n",
        "\n",
        "    parsed = parse_address(clean_addr)\n",
        "    new = normalize_directionals(parsed)\n",
        "\n",
        "    return new\n"
      ],
      "metadata": {
        "id": "gZ-xsbJQ0B2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "cols = ['BUYER ADDRESS', 'SELLER ADDRESS']\n",
        "\n",
        "for col in cols:\n",
        "\n",
        "  parsed_addr = []\n",
        "  for addr in df[col]:\n",
        "      if pd.notna(addr):\n",
        "\n",
        "        parsed_result = parse_with_libpostal(str(addr))\n",
        "        reversed_dict = {v:k for k,v in dict(parsed_result).items()}\n",
        "        parsed_addr.append(reversed_dict)\n",
        "\n",
        "      else:\n",
        "        parsed_addr.append({})\n",
        "\n",
        "  if col == 'BUYER ADDRESS':\n",
        "    parsed_buyer_df = pd.DataFrame(parsed_addr)\n",
        "  else:\n",
        "    parsed_property_df = pd.DataFrame(parsed_addr)\n",
        "\n"
      ],
      "metadata": {
        "id": "z9UPK3y10CdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_buyer_df = parsed_buyer_df.add_prefix(\"buyer_\")\n",
        "parsed_property_df = parsed_property_df.add_prefix(\"property_\")\n",
        "\n",
        "final_df = pd.concat([df, parsed_buyer_df, parsed_property_df], axis=1)\n",
        "output_path = \"/content/post_processed_predictions_2000.xlsx\"\n",
        "final_df.to_excel(output_path, index=False)\n"
      ],
      "metadata": {
        "id": "BLS2OdmN0Hn9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}